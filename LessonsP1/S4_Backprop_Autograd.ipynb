{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример прямого и обратного распространения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward\n",
      "Backward\n",
      "Loss: 1.2191184759140015\n",
      "loss_grad: 1.0\n",
      "y_p_grad: tensor([ 1.5615, -1.5615])\n",
      "z2_grad: tensor([ 1.5615, -1.5615])\n",
      "z1_grad: tensor([ 0.5346, -0.5346])\n",
      "w_grad: tensor([[ 0.1604,  0.1604,  0.4811],\n",
      "        [-0.1604, -0.1604, -0.4811]])\n",
      "b_grad: tensor([ 0.5346, -0.5346])\n"
     ]
    }
   ],
   "source": [
    "class TransparentNet(nn.Module):\n",
    "\n",
    "    def __init__(self, w: torch.Tensor, b: torch.Tensor):\n",
    "\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(w)\n",
    "        self.b = nn.Parameter(b)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self.z1 = self.w @ x + self.b\n",
    "        self.z1.retain_grad()\n",
    "\n",
    "        self.z2 = self.softmax(self.z1)\n",
    "        self.z2.retain_grad()\n",
    "\n",
    "        return self.z2\n",
    "\n",
    "w = torch.tensor([[0.2, 0.4, 0.9],\\\n",
    "                  [0.1,-0.2,-0.5]])\n",
    "\n",
    "b = torch.tensor([0.0, 0.2]).T\n",
    "\n",
    "x = torch.tensor([0.3, 0.3, 0.9]).T\n",
    "\n",
    "y = torch.tensor([0.0, 1.0])\n",
    "\n",
    "net = TransparentNet(w=w, b=b)\n",
    "\n",
    "print('Forward')\n",
    "y_p = net(x)\n",
    "dy = y - y_p\n",
    "y_p.retain_grad()\n",
    "\n",
    "loss = torch.pow(dy, 2).sum()\n",
    "loss.retain_grad()\n",
    "\n",
    "print('Backward')\n",
    "loss.backward()\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'loss_grad: {loss.grad}')\n",
    "print(f'y_p_grad: {y_p.grad}')\n",
    "print(f'z2_grad: {net.z2.grad}')\n",
    "print(f'z1_grad: {net.z1.grad}')\n",
    "print(f'w_grad: {net.w.grad}')\n",
    "print(f'b_grad: {net.b.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Автоматическое дифференцирование в PyTorch\n",
    "\n",
    "Автоматическое дифференцирование в основе своей просто:\n",
    "1. Правило композиции определяет, как перейти от одного слоя (математической операции) к другому;\n",
    "2. Строится динамический граф вычислений;\n",
    "3. Слои (математические операции) дифференцируются благодаря тому, что для всех элементарных операций определены методы вычисления производных;\n",
    "\n",
    "Реализованный выше и разобранный на занятии пример вполне иллюстрирует все эти этапы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*НЛО*:\n",
    "В тензорном исчислении, тензор можно представить следующей формулой:\n",
    "\n",
    "$$ \\texttt{тензор} = \\text{линейный оператор} + \\text{система координат} + \\text{закон его преобразования при замене координат} $$\n",
    "\n",
    "Тензоры в `pytorch` не являются тензорами в этом смысле слова. Но \"формула\" `torch.tensor` очень похожа, потому заимствование этого названия вполне уместно:\n",
    "\n",
    "$$ \\texttt{torch.tensor} = \\text{линейный оператор} + \\text{граф вычислений} + \\text{метод дифференцирования} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
