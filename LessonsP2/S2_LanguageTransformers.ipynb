{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd607a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from typing import Optional, Dict, List, Tuple, Iterable, Sized\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchinfo\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "\n",
    "# –ó–∞—Ñ–∏–∫—Å–∏—Ä—É–µ–º –∑–µ—Ä–Ω–∞\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4d884",
   "metadata": {},
   "source": [
    "# 1. PyTorch Lightning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3087b2",
   "metadata": {},
   "source": [
    "Pytorch Lightning - —ç—Ç–æ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ API –¥–ª—è PyTorch, —É–ø—Ä–æ—â–∞—é—â–µ–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π.\n",
    "\n",
    "–ï—Å—Ç—å 2 –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç—Ç–æ–≥–æ API:\n",
    "1. –ß–µ—Ä–µ–∑ –±–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã LightningModule, LightningDataModule –∏ Trainer;\n",
    "2. –ß–µ—Ä–µ–∑ —Ñ–∞–±—Ä–∏–∫—É lightinig.Fabric;\n",
    "\n",
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0655087e",
   "metadata": {},
   "source": [
    "| –°–≤–æ–π—Å—Ç–≤–æ           | Classic Lightning | Lightning Fabric |\n",
    "| ----------------- | --- | --- |\n",
    "| **–¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è** | –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç `training_step` –∏ –ø–µ—Ä–µ–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ callbacks –≤ `Trainer` | –ü–æ–ª–Ω–æ—Å—Ç—å—é –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ–ª—å–∑–æ–≤–∞–µ—Ç–µ–ª–µ–º |\n",
    "| **–û—Ç–ª–∏—á–∏—è –≤ –∫–æ–¥–µ**  | –¢—Ä–µ–±—É–µ—Ç –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∫–ª–∞—Å—Å–∞ –º–æ–¥–µ–ª–∏ –æ—Ç `LightningModule` –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–µ—Ç–æ–¥–æ–≤ | –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã - –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –æ–±–µ—Ä–Ω—É—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã |\n",
    "| **–ì–∏–±–∫–æ—Å—Ç—å**   | –ú–µ–Ω—å—à–µ –≥–∏–±–∫–æ—Å—Ç–∏, `Trainer` —É–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ–º | –¢–∞–∫–∞—è –∂–µ, –∫–∞–∫ –≤ PyTorch |\n",
    "| **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?**  | –ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏ | –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ –≥–∏–±–∫–æ—Å—Ç—å –∏–ª–∏ –Ω—É–∂–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º –ø—Ä–æ–µ–∫—Ç–µ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6a57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import lightning as L\n",
    "\n",
    "class SimpleFFN_Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, n_classes, hidden_dim, n_hidden_layers=1, activation=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim)] * n_hidden_layers, \n",
    "        )\n",
    "        self.output = nn.Linear(hidden_dim, n_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.activation(self.input(x))\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        probs = self.softmax(self.output(x))\n",
    "\n",
    "        return probs\n",
    "\n",
    "class WrappedModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, n_classes, hidden_dim, n_hidden_layers=1, activation=nn.ReLU(), lr=1e-3):\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.classifier = SimpleFFN_Classifier(input_dim, n_classes, hidden_dim, n_hidden_layers=1, activation=nn.ReLU())\n",
    "        self.loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    def training_step(self, batch, batch_idx, loader_idx=0):\n",
    "        x, labels = batch\n",
    "        probs = self.classifier(x)\n",
    "\n",
    "        loss = self.loss_fn(probs, labels)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.get('lr', 3e-4))\n",
    "    \n",
    "model = WrappedModel(10, 10, 16, 4)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    fast_dev_run=1,\n",
    "    max_time={},\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model, \n",
    "    train_dataloaders=None,\n",
    "    val_dataloaders=None,\n",
    "    datamodule=None,\n",
    "    ckpt_path=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on 4 GPUs\n",
    "trainer = L.Trainer(\n",
    "    devices=4,\n",
    "    accelerator=\"gpu\",\n",
    ")\n",
    "\n",
    "# 20+ helpful flags for rapid idea iteration\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=10,\n",
    "    min_epochs=5,\n",
    "    overfit_batches=1\n",
    ")\n",
    "\n",
    "# access the state of the art techniques\n",
    "from lightning.pytorch.callbacks import StochasticWeightAveraging\n",
    "trainer = L.Trainer(callbacks=[StochasticWeightAveraging(swa_lrs=3e-2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63cceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # called once on 1 device\n",
    "        df = pd.read_hdf('datasets/example.h5')\n",
    "        self.data = Dataset()\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # called on each device, \n",
    "        match stage:\n",
    "            case 'fit':\n",
    "                self.train = LoadTrainData()\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test)\n",
    "\n",
    "    def on_exception(self, exception: BaseException) -> None:\n",
    "        pass\n",
    "\n",
    "    def teardown(self, stage: str) -> None:\n",
    "        return super().teardown(stage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9dad2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleFFN_Classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m fabric = L.Fabric()\n\u001b[32m      5\u001b[39m fabric.launch()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mSimpleFFN_Classifier\u001b[49m()\n\u001b[32m      8\u001b[39m optimizer = torch.optim.SGD(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m      9\u001b[39m model, optimizer = fabric.setup(model, optimizer)\n",
      "\u001b[31mNameError\u001b[39m: name 'SimpleFFN_Classifier' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "fabric = L.Fabric()\n",
    "\n",
    "model = SimpleFFN_Classifier()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
    "dataloader = fabric.setup_dataloaders(dataloader)\n",
    "\n",
    "def train_loop():\n",
    "    model.train()\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "            fabric.backward(loss)\n",
    "            optimizer.step()\n",
    "            print(loss.data)\n",
    "\n",
    "fabric.launch(train_loop, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bec61",
   "metadata": {},
   "source": [
    " - **`Fabric(...)`**: –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä. –ó–¥–µ—Å—å –≤—ã –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.\n",
    "  - `accelerator`: 'cpu', 'cuda', 'mps', 'tpu', 'auto'.\n",
    "  - `devices`: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤, —Å–ø–∏—Å–æ–∫ ID —É—Å—Ç—Ä–æ–π—Å—Ç–≤ –∏–ª–∏ 'auto'.\n",
    "  - `strategy`: 'ddp', 'deepspeed' –∏ —Ç.–¥.\n",
    "  - `precision`: '64', '32', '16-mixed', 'bf16-mixed'.\n",
    "  \n",
    "- **`fabric.setup(model, optimizer)`**: –û–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä. –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤—ã–∑–≤–∞–Ω –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –æ–±—É—á–µ–Ω–∏—è.\n",
    "  \n",
    "- **`fabric.setup_dataloaders(...)`**: –û–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö.\n",
    "  \n",
    "- **`fabric.backward(loss)`**: –ó–∞–º–µ–Ω–∞ `loss.backward()`. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.\n",
    "  \n",
    "- **`fabric.print(...)`**: –ó–∞–º–µ–Ω–∞ `print()`. –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –≤—ã–≤–æ–¥ —Å–æ–æ–±—â–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ (—Ä–∞–Ω–≥ 0).\n",
    "  \n",
    "- **`fabric.launch(function, *args)`**: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± –∑–∞–ø—É—Å–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è. –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞.\n",
    "  \n",
    "- **`fabric.save(path, state)`**: –£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫, –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞—é—â–∞—è –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å—Ä–µ–¥–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.\n",
    "  \n",
    "- **`fabric.all_gather(tensor)`**: –°–±–æ—Ä —Ç–µ–Ω–∑–æ—Ä–∞ —Å–æ –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤. –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee7323",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run on 2 GPUs with DDP strategy\n",
    "fabric run main_fabric.py --accelerator=cuda --devices=2\n",
    "\n",
    "# Run with 16-bit mixed precision\n",
    "fabric run main_fabric.py --accelerator=cuda --devices=1 --precision=16-mixed\n",
    "\n",
    "# Run on CPU\n",
    "fabric run main_fabric.py --accelerator=cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8696747",
   "metadata": {},
   "source": [
    "# 2. –†–µ–∞–ª–∏–∑—É–µ–º baseline –æ–±—É—á–µ–Ω–∏—è —Ç—Ä–∞—Å—Ñ–æ—Ä–º–µ—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce92b627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/miniconda3/envs/nn_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tokenizer of\n",
      "Vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "model = AutoModel.from_pretrained(\"google/mobilebert-uncased\")\n",
    "embeddings = deepcopy(model.embeddings)\n",
    "# https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mobilebert-uncased\")\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "print(f'Using tokenizer of')\n",
    "print(f'Vocabulary size: {VOCAB_SIZE}')\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            path: str,\n",
    "            tokenizer: AutoTokenizer,\n",
    "            source_columns: List[str],\n",
    "            target_columns: List[str] = None,\n",
    "            nrows = None,\n",
    "            max_len = 256,\n",
    "            padding_type = \"max_length\",\n",
    "            ):\n",
    "\n",
    "        self.source_columns = source_columns\n",
    "        self.target_columns = target_columns if target_columns else source_columns\n",
    "\n",
    "        columns = source_columns\n",
    "        if target_columns:\n",
    "            columns += target_columns\n",
    "        \n",
    "        data = pd.read_csv(path, usecols=list(set(columns)), nrows=nrows)\n",
    "\n",
    "        self.src_ = []\n",
    "        self.tgt_ = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.CLS_id = tokenizer.cls_token_id\n",
    "        self.SEP_id = tokenizer.sep_token_id\n",
    "        self.PAD_idx = tokenizer.pad_token_id\n",
    "        self.max_len = max_len\n",
    "        self.padding_type = padding_type\n",
    "\n",
    "        for i in tqdm.trange(len(data)):\n",
    "            row = data.iloc[i]\n",
    "            src_text = self.process_row(row, source_columns)\n",
    "            self.src_.append(src_text)\n",
    "            \n",
    "            if target_columns is not None:\n",
    "                tgt_text = self.process_row(row, target_columns) \n",
    "                self.tgt_.append(tgt_text)\n",
    "        \n",
    "        if target_columns is None:\n",
    "            self.tgt_ = self.src_\n",
    "        \n",
    "        self.size = len(self.src_)\n",
    "\n",
    "    def process_row(self, row: pd.Series, columns: List[str]):\n",
    "        \"\"\"Processes a single recipe row from the DataFrame into a clean string.\"\"\"\n",
    "        entry_parts = []\n",
    "        for col in columns:\n",
    "            if pd.notna(row[col]):\n",
    "                content = str(row[col])\n",
    "                if content.startswith('[') and content.endswith(']'):\n",
    "                    content = re.sub(r'[\"\\\\$$\\\\\\\\$$]', '', content)\n",
    "                entry_parts.append(f'{col.replace(\"_\", \" \")}: {content}')\n",
    "                entry_parts.append('\\n')\n",
    "        return ''.join(entry_parts[:-1])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[str, str]:\n",
    "        source = self.src_[idx]\n",
    "        target = self.tgt_[idx]\n",
    "        return source, target\n",
    "\n",
    "    def single(self, idx):\n",
    "        return self.collate_fn([(self.src_[idx], self.tgt_[idx])])\n",
    "\n",
    "    def collate_fn(self, batch: List[Tuple[str, str]]) -> Dict[str, List]:\n",
    "        \n",
    "        sources = [f[0] for f in batch]\n",
    "        targets = [f[1] for f in batch]\n",
    "    \n",
    "        source_enc = self.tokenizer(list(sources), max_length=self.max_len, \n",
    "                                    padding=self.padding_type, truncation=True, \n",
    "                                    return_tensors=\"pt\").to(device=self.device)\n",
    "\n",
    "        target_enc = self.tokenizer(list(targets), max_length=self.max_len,\n",
    "                                    padding=self.padding_type, truncation=True, \n",
    "                                    return_tensors=\"pt\").to(device=self.device)\n",
    "        return {\n",
    "            'input_ids': source_enc['input_ids'], \n",
    "            'input_mask': source_enc['attention_mask'],\n",
    "            'input_lengths': source_enc['attention_mask'].sum(dim=-1),\n",
    "            'labels': target_enc['input_ids'],\n",
    "            'labels_mask': target_enc['attention_mask'],\n",
    "            'labels_lengths': target_enc['attention_mask'].sum(dim=-1),   \n",
    "            }\n",
    "\n",
    "    def get_loaders(\n",
    "            self,\n",
    "            names: Optional[List[str]] = ['train', 'val'],\n",
    "            ratios: Optional[List[float]] = [0.9, 0.1],\n",
    "            shuffle: Optional[List[bool]] = [True, False],\n",
    "            batch_size: int = 8,\n",
    "            load_ratio: int = 1.0,\n",
    "            **kwargs,\n",
    "        ) -> Dict[str, DataLoader]:\n",
    "        \"\"\"\n",
    "        Fetches several dataloaders from this dataset\n",
    "        \"\"\" \n",
    "\n",
    "        indices = list(range(len(self)))\n",
    "        i0 = 0\n",
    "        dataloaders: Dict[str, DataLoader] = {}\n",
    "        \n",
    "        for name, part, shuff in zip(names, ratios, shuffle):\n",
    "            part_len = int(len(indices) * part * load_ratio )\n",
    "            subset = Subset(self, indices[i0: i0 + part_len])\n",
    "            dataloaders[name] = DataLoader(subset, batch_size, shuff, collate_fn=self.collate_fn, **kwargs)\n",
    "            i0 += part_len        \n",
    "            \n",
    "        return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5fbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/1_Recipe_csv.csv'\n",
    "NSAMPLES = 48000\n",
    "\n",
    "copy_dataset = RecipeDataset(DATA_PATH, tokenizer, ['description', 'ingredients'], None,\\\n",
    "                     nrows=NSAMPLES, padding_type=\"longest\", device='cpu')\n",
    "\n",
    "ingredients_dataset = RecipeDataset(DATA_PATH, tokenizer, ['recipe_title', 'description'], ['ingredients'],\\\n",
    "                     nrows=NSAMPLES, padding_type=\"longest\", device='cpu')\n",
    "\n",
    "recipe_dataset = RecipeDataset(DATA_PATH, tokenizer, ['recipe_title', 'description'], ['ingredients'],\\\n",
    "                     nrows=NSAMPLES, padding_type=\"longest\", device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd04f81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 102, 0, 101, 103]\n",
      "{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.all_special_ids)\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b72d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–µ–∞–ª–∏–∑—É–π—Ç–µ –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–∏\n",
    "from typing import Any\n",
    "import lightning as L\n",
    "\n",
    "class RecipeTransformer(L.LightningModule):\n",
    "\n",
    "    def __init__(self, embeddings: nn.Embedding, n_layers: int, MHAnheads: int, ff_dim: int, dropout: float, ):\n",
    "\n",
    "        self.emb_dim = embeddings.embedding_size\n",
    "        self.vocab_size = embeddings.word_embeddings.num_embeddings\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.emb_dim, nhead=MHAnheads, dim_feedforward=ff_dim, dropout=dropout,\n",
    "            norm_first=True, batch_first=True,\n",
    "            )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=self.emb_dim, nhead=MHAnheads, dim_feedforward=ff_dim, dropout=dropout,\n",
    "            norm_first=True, batch_first=True,\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=n_layers)\n",
    "\n",
    "        self.projection = nn.Linear(self.emb_dim, self.vocab_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.bos_id = torch.IntTensor([102])\n",
    "        self.eos_id = torch.IntTensor([101])\n",
    "\n",
    "        # \n",
    "        self.loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.tf_ratio = 0.9\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor=None, nsteps: int = None, tf_ratio: float =0.9):\n",
    "        '''\n",
    "        Args:\n",
    "            - src: torch.Tensor [B, L] - input sequence\n",
    "            - tgt: torch.Tensor [B, T] - target sequence ()\n",
    "        '''\n",
    "        x = self.embedding(src)\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        context = self.encoder(x)\n",
    "\n",
    "        if tgt is not None:\n",
    "            nsteps = tgt.shape[-1]\n",
    "        elif nsteps is not None:\n",
    "            pass\n",
    "        else:\n",
    "            nsteps = 1\n",
    "\n",
    "        outp = []\n",
    "        y = self.embedding(self.bos_id)\n",
    "        for i in range(nsteps):\n",
    "            y = self.decoder.forward(memory=context, tgt=y)\n",
    "            outp.append(y.view(batch_size, 1, self.emb_dim))\n",
    "            if self.training and tgt is not None:\n",
    "                tf_mask = torch.randn(batch_size) > tf_ratio\n",
    "                y[tf_mask] = tgt[tf_mask, i]\n",
    "\n",
    "        outp = torch.cat(outp, dim=1)\n",
    "\n",
    "        return self.softmax(self.projection(outp))\n",
    "    \n",
    "    def training_step(self, batch, batch_id, dataloader_id=0):\n",
    "\n",
    "        src = batch['input_ids']\n",
    "        tgt = batch['labels']\n",
    "\n",
    "        pred = self.forward(src, tgt, self.tf_ratio)\n",
    "\n",
    "        loss = self.loss_fn(pred, tgt)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        return torch.optim.Adam(self.parameters(), lr=3e-4)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4002ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Recipe:\n",
    "    src: torch.Tensor\n",
    "    tgt: torch.Tensor\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52387059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95e6d72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__constants__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_fill_padding_idx_with_zero',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'embedding_dim',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'max_norm',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'norm_type',\n",
       " 'num_embeddings',\n",
       " 'padding_idx',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'scale_grad_by_freq',\n",
       " 'set_extra_state',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'sparse',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(embeddings.word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c29291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é L.Trainer()\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
