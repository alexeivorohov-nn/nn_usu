{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd607a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Optional, Dict, List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c4953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "model = AutoModel.from_pretrained(\"google/mobilebert-uncased\")\n",
    "embedding = deepcopy(model.embeddings)\n",
    "# https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mobilebert-uncased\")\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "print(f'Using tokenizer of')\n",
    "print(f'Vocabulary size: {VOCAB_SIZE}')\n",
    "del models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65980a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class RecipeDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self, path: str,\n",
    "            source_columns: List[str],\n",
    "            target_columns: List[str],\n",
    "            tokenizer: AutoTokenizer,\n",
    "            nrows = None,\n",
    "            device = 'cpu',\n",
    "            max_len = 256,\n",
    "            padding_type = \"max_length\",\n",
    "            ):\n",
    "\n",
    "        columns = source_columns + target_columns\n",
    "        self.source_columns = source_columns\n",
    "        self.target_columns = target_columns\n",
    "\n",
    "        data = pd.read_csv(path, usecols=columns, nrows=nrows)\n",
    "\n",
    "        self.src_ = []\n",
    "        self.tgt_ = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.PAD_idx = tokenizer.pad_token_id\n",
    "        self.max_len = max_len\n",
    "        self.padding_type = padding_type\n",
    "\n",
    "        for i in tqdm.trange(len(data)):\n",
    "            row = data.iloc[i]\n",
    "            src_text = self.process_row(row, source_columns)\n",
    "            tgt_text = self.process_row(row, target_columns)\n",
    "            \n",
    "            self.src_.append(src_text)\n",
    "            self.tgt_.append(tgt_text)\n",
    "\n",
    "        self.size = len(self.src_)\n",
    "\n",
    "    def process_row(self, row: pd.Series, columns: List[str]):\n",
    "        \"\"\"Processes a single recipe row from the DataFrame into a clean string.\"\"\"\n",
    "        entry_parts = []\n",
    "        for col in columns:\n",
    "            if pd.notna(row[col]):\n",
    "                content = str(row[col])\n",
    "                if content.startswith('[') and content.endswith(']'):\n",
    "                    content = re.sub(r'[\"\\\\$$\\\\\\\\$$]', '', content)\n",
    "                entry_parts.append(f'{col.replace(\"_\", \" \")}: {content}')\n",
    "                entry_parts.append('\\n')\n",
    "        return ''.join(entry_parts[:-1])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[str, str]:\n",
    "        source = self.src_[idx]\n",
    "        target = self.tgt_[idx]\n",
    "        return source, target\n",
    "\n",
    "    def single(self, idx):\n",
    "        return self.collate_fn([(self.src_[idx], self.tgt_[idx])])\n",
    "\n",
    "    def collate_fn(self, batch: List[Tuple[str, str]]) -> Dict[str, List]:\n",
    "        \n",
    "        sources = [f[0] for f in batch]\n",
    "        targets = [f[1] for f in batch]\n",
    "    \n",
    "        source_enc = self.tokenizer(list(sources), max_length=self.max_len, \n",
    "                                    padding=self.padding_type, truncation=True, \n",
    "                                    return_tensors=\"pt\", return_length=True)\n",
    "\n",
    "        target_enc = self.tokenizer(list(targets), max_length=self.max_len,\n",
    "                                    padding=self.padding_type, truncation=True, \n",
    "                                    return_tensors=\"pt\", return_length=True)\n",
    "\n",
    "        batch = {\n",
    "            'input_ids': source_enc['input_ids'], \n",
    "            'attention_mask': source_enc['attention_mask'],\n",
    "            'input_lengths': source_enc['length'],\n",
    "            'labels': target_enc['input_ids'],\n",
    "            'labels_lengths': target_enc['length'],\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    def get_dataloaders(\n",
    "            self,\n",
    "            names: Optional[List[str]] = ['train', 'val'],\n",
    "            ratios: Optional[List[float]] = [0.9, 0.1],\n",
    "            shuffle: Optional[List[bool]] = [True, False],\n",
    "            batch_size: int = 8,\n",
    "            load_ratio: int = 1.0,\n",
    "            **kwargs,\n",
    "        ) -> Dict[str, DataLoader]:\n",
    "        \"\"\"\n",
    "        Fetches several dataloaders from this dataset\n",
    "        \"\"\" \n",
    "\n",
    "        indices = list(range(len(self)))\n",
    "        i0 = 0\n",
    "        dataloaders: Dict[str, DataLoader] = {}\n",
    "        \n",
    "        for name, part, shuff in zip(names, ratios, shuffle):\n",
    "            part_len = int(len(indices) * part * load_ratio )\n",
    "            subset = Subset(self, indices[i0: i0 + part_len])\n",
    "            dataloaders[name] = DataLoader(subset, batch_size, shuff, collate_fn=data.collate_fn, **kwargs)\n",
    "            i0 += part_len        \n",
    "            \n",
    "        return dataloaders\n",
    "\n",
    "DATA_PATH = '../data/1_Recipe_csv.csv'\n",
    "INPUT_COLS = ['recipe_title', 'category']\n",
    "TARGET_COLS = ['description', 'ingredients', 'directions']\n",
    "NSAMPLES = 1024\n",
    "\n",
    "dataset = RecipeDataset(DATA_PATH, INPUT_COLS, TARGET_COLS,\\\n",
    "                     tokenizer, nrows=NSAMPLES, padding_type=\"longest\")\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = dataset.get_dataloaders(\n",
    "    ['train', 'test', 'val'], [0.8, 0.1, 0.1], [True, False, False], batch_size=8).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4d884",
   "metadata": {},
   "source": [
    "## PyTorch Lightning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f01d975",
   "metadata": {},
   "source": [
    "## Optuna\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d03971",
   "metadata": {},
   "source": [
    "## Tensorboard integration with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b72d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeTransformer(nn,Module):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c29291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4287e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "# import torchvision as tv\n",
    "\n",
    "dataset = tv.datasets.CIFAR10(\"data\", download=True,\n",
    "                              train=True,\n",
    "                              transform=tv.transforms.ToTensor())\n",
    "\n",
    "fabric = L.Fabric()\n",
    "fabric.launch()\n",
    "\n",
    "model = tv.models.resnet18()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
    "dataloader = fabric.setup_dataloaders(dataloader)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        fabric.backward(loss)\n",
    "        optimizer.step()\n",
    "        print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd57e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
