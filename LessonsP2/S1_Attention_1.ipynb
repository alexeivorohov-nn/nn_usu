{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Iterable, Sized\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")\n",
    "\n",
    "# Зафиксируем зерна\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "class logger:\n",
    "    active = False\n",
    "    _calls_ = {}\n",
    "    log_file = \"logger_output.txt\"  # Default log file path\n",
    "    silent = True\n",
    "\n",
    "    @classmethod\n",
    "    def on(cls): cls.active = True\n",
    "\n",
    "    @classmethod\n",
    "    def off(cls): cls.active = False\n",
    "\n",
    "    @classmethod\n",
    "    def silent(cls, silent: bool = True):\n",
    "        cls.silent = silent\n",
    "\n",
    "    @classmethod\n",
    "    def zero(cls):\n",
    "        cls._calls_ = {}\n",
    "\n",
    "    @classmethod\n",
    "    def clear_log(cls):\n",
    "        with open(cls.log_file, 'w') as f:\n",
    "            f.write(\"\")\n",
    "\n",
    "    @classmethod\n",
    "    def write_log(cls, msg):\n",
    "        with open(cls.log_file, 'a') as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "\n",
    "    @classmethod\n",
    "    def trace(cls, name):\n",
    "        def log_fn(func):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                if cls.active:\n",
    "                    if name not in cls._calls_:\n",
    "                        cls._calls_[name] = 0\n",
    "                    msg = f'>>> {name} call {cls._calls_[name]}: \\n Args: \\n'\n",
    "                    for i, arg in enumerate(args):\n",
    "                        if isinstance(arg, torch.Tensor):\n",
    "                            msg += f'\\t arg[{i}]: shape={arg.shape}, dtype={arg.dtype}, device={arg.device}\\n {arg} \\n'\n",
    "                        else:\n",
    "                            msg += f'\\t arg[{i}]: {arg}\\n'\n",
    "\n",
    "                    for k, arg in kwargs.items():\n",
    "                        if isinstance(arg, torch.Tensor):\n",
    "                            msg += f'\\t kwarg[{k}]: shape={arg.shape}, dtype={arg.dtype}, device={arg.device}\\n {arg} \\n'\n",
    "                        else:\n",
    "                            msg += f'\\t kwarg[{k}]: {arg} \\n'\n",
    "\n",
    "                    if not cls.silent: print(msg)\n",
    "                    cls.write_log(msg)\n",
    "\n",
    "                result = func(*args, **kwargs)\n",
    "\n",
    "                if cls.active:\n",
    "                    msg = f'Result: \\n'\n",
    "                    if isinstance(result, Iterable):\n",
    "                        for i, outp in enumerate(result):\n",
    "                            if isinstance(outp, torch.Tensor):\n",
    "                                msg += f'\\t output[{i}]: shape={outp.shape}, dtype={outp.dtype}, device={outp.device}\\n {outp} \\n'\n",
    "                            else:\n",
    "                                msg += f'\\t output[{i}]: {outp}\\n'\n",
    "                    elif isinstance(result, torch.Tensor):\n",
    "                        msg = f'\\t output: shape={result.shape}, dtype={result.dtype}, device={result.device}\\n {result} \\n'\n",
    "                    else:\n",
    "                        msg += f'\\t output: {result}\\n'\n",
    "\n",
    "                    if not cls.silent: print(msg)\n",
    "                    cls.write_log(msg)\n",
    "                    cls._calls_[name] += 1\n",
    "\n",
    "                return result\n",
    "            return wrapper\n",
    "\n",
    "        return log_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE3a4sgdCQPr"
   },
   "source": [
    "# 1. Токенизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "# https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mobilebert-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "print(f'Vocabulary size: {VOCAB_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_special_tokens(tokenizer: AutoTokenizer) -> Dict:\n",
    "\n",
    "    special_tokens = ['PAD', 'BOS', 'EOS', 'UNK', 'SEP', 'CLS']\n",
    "    outp = {}\n",
    "    for name in special_tokens:\n",
    "        token = getattr(tokenizer, f'{name.lower()}_token')\n",
    "        if token is not None:\n",
    "            outp[name] = token\n",
    "            outp[f'{name}_id'] = getattr(tokenizer, f'{name.lower()}_token_id')\n",
    "        else:\n",
    "            print(f'{name} token  is not present in tokenizer')\n",
    "    \n",
    "    for name in special_tokens:\n",
    "        if name in outp:\n",
    "            print(f'{name} token: token {outp[name]}, id {outp[f'{name}_id']}')\n",
    "\n",
    "    return outp\n",
    "\n",
    "special_tokens = collect_special_tokens(tokenizer)\n",
    "\n",
    "for k, v in special_tokens.items():\n",
    "    globals()[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'NLP (Neuro-Linguistic Programming) is a psychological approach that involves analyzing the patterns of thought, language, and behavior to understand how they interact with and influence human experience. There is no scientific evidence supporting the effectiveness of NLP; it is recognized as a pseudoscience.'\n",
    "# apply tokenizer:\n",
    "seq = tokenizer(text)\n",
    "print(seq)\n",
    "# decode with tokenizer:\n",
    "print(tokenizer.decode(seq['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['NLP (Neuro-Linguistic Programming) is a psychological approach that involves analyzing the patterns of thought, language, and behavior to understand how they interact with and influence human experience.',\n",
    "         'There is no scientific evidence supporting the effectiveness of NLP; it is recognized as a pseudoscience.']\n",
    "seq = tokenizer(text, max_length=64, padding=\"max_length\",\\\n",
    "                truncation=\"longest_first\", return_tensors=\"pt\",\\\n",
    "                return_token_type_ids = False, return_length=True)\n",
    "print(seq)\n",
    "# decode with tokenizer:\n",
    "print(tokenizer.decode(seq['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возвращается не словарь, а свой тип, имеющий  методы Dict() и другие \n",
    "type(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*seq.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(''.join([_attr_+'   ' for _attr_ in dir(seq)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjcEKe88CU6I"
   },
   "source": [
    "# 2. Данные и контейнеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "data_dir=\"../data\"\n",
    "csv_name=\"1_Recipe_csv.csv\"\n",
    "zip_name=\"recipes-dataset-64k.zip\"\n",
    "zip_path=\"$data_dir/$zip_name\"\n",
    "\n",
    "if [ ! -d \"$data_dir\" ]; then\n",
    "    mkdir -p \"$data_dir\"\n",
    "    echo \"Directory created: $data_dir\"\n",
    "else\n",
    "    echo \"Found data directory: $data_dir\"\n",
    "fi\n",
    "\n",
    "if [ ! -f \"$data_dir/$csv_name\" ]; then\n",
    "    echo \"CSV file $csv_name not found...\"\n",
    "    if [ ! -f \"$zip_path\" ]; then\n",
    "        echo \"Archive $zip_name not found, downloading...\"\n",
    "        curl -L -o \"$zip_path\" https://www.kaggle.com/api/v1/datasets/download/prashantsingh001/recipes-dataset-64k-dishes\n",
    "        echo \"Dataset downloaded: $zip_path\"\n",
    "    else\n",
    "        echo \"Found archive $zip_name\"\n",
    "    fi\n",
    "    echo \"Extracting to $data_dir ...\"\n",
    "    unzip \"$zip_path\" -d \"$data_dir\"\n",
    "    echo \"Dataset unzipped to: $data_dir\"\n",
    "else\n",
    "    echo \"Found dataset: $data_dir/$csv_name\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class RecipeDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self, path: str,\n",
    "            source_columns: List[str],\n",
    "            target_columns: List[str],\n",
    "            tokenizer: AutoTokenizer,\n",
    "            nrows = None,\n",
    "            device = 'cpu',\n",
    "            max_len = 256,\n",
    "            padding_type = \"max_length\",\n",
    "            ):\n",
    "\n",
    "        columns = source_columns + target_columns\n",
    "        self.source_columns = source_columns\n",
    "        self.target_columns = target_columns\n",
    "\n",
    "        data = pd.read_csv(path, usecols=columns, nrows=nrows)\n",
    "\n",
    "        self.src_ = []\n",
    "        self.tgt_ = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.PAD_idx = tokenizer.pad_token_id\n",
    "        self.max_len = max_len\n",
    "        self.padding_type = padding_type\n",
    "\n",
    "        for i in tqdm.trange(len(data)):\n",
    "            row = data.iloc[i]\n",
    "            src_text = self.process_row(row, source_columns)\n",
    "            tgt_text = self.process_row(row, target_columns)\n",
    "            \n",
    "            self.src_.append(src_text)\n",
    "            self.tgt_.append(tgt_text)\n",
    "\n",
    "        self.size = len(self.src_)\n",
    "\n",
    "    def process_row(self, row: pd.Series, columns: List[str]):\n",
    "        \"\"\"Processes a single recipe row from the DataFrame into a clean string.\"\"\"\n",
    "        entry_parts = []\n",
    "        for col in columns:\n",
    "            if pd.notna(row[col]):\n",
    "                content = str(row[col])\n",
    "                if content.startswith('[') and content.endswith(']'):\n",
    "                    content = re.sub(r'[\"\\\\$$\\\\\\\\$$]', '', content)\n",
    "                entry_parts.append(f'{col.replace(\"_\", \" \")}: {content}')\n",
    "                entry_parts.append('\\n')\n",
    "        return ''.join(entry_parts[:-1])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[str, str]:\n",
    "        source = self.src_[idx]\n",
    "        target = self.tgt_[idx]\n",
    "        return source, target\n",
    "\n",
    "    def single(self, idx):\n",
    "        return self.collate_fn([(self.src_[idx], self.tgt_[idx])])\n",
    "\n",
    "    def collate_fn(self, batch: List[Tuple[str, str]]) -> Dict[str, List]:\n",
    "        \n",
    "        sources = [f[0] for f in batch]\n",
    "        targets = [f[1] for f in batch]\n",
    "    \n",
    "        source_enc = self.tokenizer(list(sources), max_length=self.max_len, \n",
    "                                    padding=self.padding_type, truncation=True, \n",
    "                                    return_tensors=\"pt\", return_length=True)\n",
    "\n",
    "        target_enc = self.tokenizer(list(targets), max_length=self.max_len,\n",
    "                                    padding=self.padding_type, truncation=True, \n",
    "                                    return_tensors=\"pt\", return_length=True)\n",
    "\n",
    "        batch = {\n",
    "            'input_ids': source_enc['input_ids'], \n",
    "            'attention_mask': source_enc['attention_mask'],\n",
    "            'input_lengths': source_enc['length'],\n",
    "            'labels': target_enc['input_ids'],\n",
    "            'labels_lengths': target_enc['length'],\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "DATA_PATH = '../data/1_Recipe_csv.csv'\n",
    "INPUT_COLS = ['recipe_title', 'category']\n",
    "TARGET_COLS = ['description', 'ingredients', 'directions']\n",
    "NSAMPLES = 1024\n",
    "\n",
    "data = RecipeDataset(DATA_PATH, INPUT_COLS, TARGET_COLS,\\\n",
    "                     tokenizer, nrows=NSAMPLES, padding_type=\"longest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, NSAMPLES)\n",
    "print(data[idx][0])\n",
    "print(data[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.single(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "def get_dataloaders(dataset: Dataset, collate_fn=None, train_ratio=0.8, val_ratio=0.1, batch_size=1):\n",
    "    \"\"\"\n",
    "    Loads data, splits it, and creates train, validation, and test DataLoaders.\n",
    "    \"\"\"\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    train_end = int(len(indices) * train_ratio)\n",
    "    val_end = int(len(indices) * (train_ratio + val_ratio))\n",
    "\n",
    "    train_set = Subset(dataset, indices[:train_end])\n",
    "    val_set = Subset(dataset, indices[train_end:val_end])\n",
    "    test_set = Subset(dataset, indices[val_end:])\n",
    "\n",
    "    print(f\"Data split:\")\n",
    "    print(f\"Training set size: {len(train_set)}\")\n",
    "    print(f\"Validation set size: {len(val_set)}\")\n",
    "    print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "    if batch_size >= 1:\n",
    "        print(\"Using batching with padding. Ensure your training loop can handle batched data!\")\n",
    "\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    val_loader = DataLoader(val_set, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    test_loader = DataLoader(test_set, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    print(f\"DataLoaders created with batch size {batch_size}.\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(data, collate_fn=data.collate_fn, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_n = 1\n",
    "n = 0\n",
    "\n",
    "for X in test_loader:\n",
    "    if n == batch_n:\n",
    "         print(X)\n",
    "    elif n > batch_n:\n",
    "        break\n",
    "    n += 1      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pgpvoyMCuMq"
   },
   "source": [
    "# 3.1. Seq2seq без внимания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDTYPE = torch.int32\n",
    "FDTYPE = torch.float32\n",
    "# FDTYPE = torch.float8_e5m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bidirectional GRU to sequence of embeddings x.\n",
    "        The input mini-batch x needs to be sorted by length.\n",
    "        x should have dimensions [batch, time, dim].\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output, hidden = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "                 \n",
    "        self.rnn = nn.GRU(input_size + 2*hidden_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "                 \n",
    "        # Для инициализации из конечного состояния энкодера\n",
    "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, inputs, encoder_hidden, hidden=None, max_len=None):\n",
    "        \"\"\"\n",
    "        Unroll the decoder for a batch of text sequences\n",
    "\n",
    "        Args:\n",
    "            - inputs: torch.Tensor - encoder output (batch, seq_len, encoder_output)\n",
    "            - encoder_hidden: torch.Tensor - last encoder hidden state (batch, seq_len, encoder_hidden)\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE\n",
    "        \n",
    "        return decoder_states, hidden # [B, L, H]\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the initial decoder state,\n",
    "        conditioned on the final encoder state.\"\"\"\n",
    "\n",
    "        if encoder_final is None:\n",
    "            return None  \n",
    "\n",
    "        return torch.tanh(self.bridge(encoder_final))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Seq2Seq architectuew\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 embed_dim,\n",
    "                 hidden_size,\n",
    "                 n_layers,\n",
    "                 dropout,\n",
    "                 EncoderCls,\n",
    "                 DecoderCls,\n",
    "                 tie_projection=False\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - tokenizer: AutoTokenizer\n",
    "            - embed_dim: int - size of embedding dimension\n",
    "            - nlayers: number of layers in rnn\n",
    "            - cls: encoder class to use, must match signature (),\n",
    "            - decoder_cls: decoder class to use, must match signature (),\n",
    "            - tie_projection: bool - if true, embedding and projection layer will use same weights\n",
    "        \"\"\"\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embeddings(self.vocab_size, embded_dim, padding_idx=0)\n",
    "        self.encoder = EncoderCls(embed_dim, hidden_size, n_layers, dropout)\n",
    "        self.decoder = DecoderCls(hidden_size, embed_dim, n_layers, dropout)\n",
    "        self.projection = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        if tie_projection:\n",
    "            self.projection.weights = self.embedding.weights\n",
    "        \n",
    "    def forward(self, tokens, src_mask=None, src_lengths=None, tgt_mask=None, tgt_lengths=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - tokens: torch.Tensor[int] : input tokens \n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE\n",
    "        \n",
    "        \n",
    "        logits = F.softmax(projection)\n",
    "        return logits\n",
    "        \n",
    "\n",
    "    \n",
    "    def calculate_loss(self, loss_fn, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8d6zRUeDN-k"
   },
   "source": [
    "# 3.2. Seq2seq с вниманием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySfjzTvvDgBr"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # Поскольку в энкодере двунаправленный GRU, key_size = 2*hidden_size\n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size is None else query_size\n",
    "\n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        self.scores = None\n",
    "        \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\"\n",
    "\n",
    "        query = self.query_layer(query)\n",
    "        \n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        self.scores = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        context = torch.bmm(self.scores, value)\n",
    "        \n",
    "        # context shape: [B, 1, 2D], scores shape: [B, 1, M]\n",
    "        return context, self.scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуйте и обучите Seq2Seq с вниманием\n",
    "\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bp_g1_IkDrGz"
   },
   "source": [
    "# 5. Контрольные вопросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlACumeWDvxV"
   },
   "outputs": [],
   "source": [
    "# Насколько корректно модель предсказывает рецепты из тренировочной выборки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOkcdsQWD_7Y"
   },
   "outputs": [],
   "source": [
    "# Насколько корректно модель предсказывает рецепты из тестовой выборки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5subBxpEH90"
   },
   "outputs": [],
   "source": [
    "# Что будет, если подать рецепт из категорий, на которых модель не обучалась?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zthbgR3lEeao"
   },
   "outputs": [],
   "source": [
    "# Что будет, если подать промпт, не предполагающйий наличие рецепта?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
