{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*Большую часть NLP-специфичного материала (TF-IDF, BagOfWords, N-grams и т.д.) мы пропускаем. Если эти темы вам в дальнейшем понадобятся, могут быть полезны материалы*\n",
        "- *Подробнее про эмбеддинги на примере word2vec: [хабр](https://habr.com/ru/articles/446530/)*\n",
        "- *Серия лекций DLS МФТИ по NLP, начало тут: [вк](https://vk.com/video-155161349_456239178), [youtube](https://www.youtube.com/watch?v=StZaHBNWiOs)*\n",
        "- *Серия видео 3Blue1Brown по LLM: [youtube](https://www.youtube.com/watch?v=LPZh9BOjkQs&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5)*\n",
        "- *Оригинальная работа, в которой введено внимание Баданау: [arxiv](https://arxiv.org/abs/1409.0473)*\n",
        "\n",
        "*На этой лекции мы очень кратко рассмотрим токенизацию и эмбеддинги, реализуем простой текстовый классификатор на основе GRU-энкодера, а затем модифицируем его, реализовав механизм внимания Баданау.*"
      ],
      "metadata": {
        "id": "JxjfjPLlQbSf"
      },
      "id": "JxjfjPLlQbSf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UxQI_4imacga",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxQI_4imacga",
        "outputId": "ce1ea030-45ea-45d1-e8ff-8b9680315837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d59ad2c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d59ad2c1",
        "outputId": "1eeab021-7e43-4b20-cbd2-ce0010ee5d3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Используемое устройство: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Используемое устройство: {device}\")\n",
        "\n",
        "# Зафиксируем зерна\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ced533a",
      "metadata": {
        "id": "4ced533a"
      },
      "source": [
        "## 1. Токенизация и эмбеддинги\n",
        "\n",
        "`gensim` - библотека для работы с текстом и темами, в области обработки естественного языка (NLP). В частности, она позволяет широкий функционал для векторизации текста, но этим функционал не ограничивается (см. https://pypi.org/project/gensim/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05faff6b",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "05faff6b",
        "outputId": "f93eac09-9cfa-49f5-8b50-b4e8ab55270b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>gensim.models.keyedvectors.KeyedVectors</b><br/>def __init__(vector_size, count=0, dtype=np.float32, mapfile_path=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/gensim/models/keyedvectors.py</a>Serialize/deserialize objects from disk, by equipping them with the `save()` / `load()` methods.\n",
              "\n",
              "Warnings\n",
              "--------\n",
              "This uses pickle internally (among other techniques), so objects must not contain unpicklable attributes\n",
              "such as lambda functions etc.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 211);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "gensim.models.keyedvectors.KeyedVectors"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "glove_vectors = api.load('glove-wiki-gigaword-50')\n",
        "type(glove_vectors)\n",
        "# https://radimrehurek.com/gensim/models/keyedvectors.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "535d7d09",
      "metadata": {
        "id": "535d7d09"
      },
      "outputs": [],
      "source": [
        "vec1 = glove_vectors['cat']\n",
        "print(vec1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79fdbf75",
      "metadata": {
        "id": "79fdbf75"
      },
      "outputs": [],
      "source": [
        "glove_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86a73bf3",
      "metadata": {
        "id": "86a73bf3"
      },
      "outputs": [],
      "source": [
        "vec1 = - glove_vectors['men'] + glove_vectors['women']\n",
        "vec2 = glove_vectors['boy']\n",
        "glove_vectors.most_similar(vec1+vec2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f86a866",
      "metadata": {
        "id": "2f86a866"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "vec = glove_vectors['student']\n",
        "near = glove_vectors.most_similar(vec)\n",
        "words = [val[0] for val in near]\n",
        "cosine = [val[1] for val in near]\n",
        "vecs = [glove_vectors[word] for word in words]\n",
        "vecs = np.stack(vecs)\n",
        "\n",
        "labels = [f'{w}:{float(s):.2f}' for w, s in zip(words, cosine)]\n",
        "fig = plt.figure(figsize=(25, 5))\n",
        "sns.heatmap(vecs, yticklabels=labels, annot=True, fmt=\".1f\", linewidths=1, square=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9c05a3a",
      "metadata": {
        "id": "d9c05a3a"
      },
      "outputs": [],
      "source": [
        "shift = np.zeros(50)\n",
        "shift[29] = 3.0\n",
        "\n",
        "glove_vectors.most_similar(glove_vectors['human'] + shift)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b822218",
      "metadata": {
        "id": "5b822218"
      },
      "outputs": [],
      "source": [
        "# Проблема: по умолчанию вектора для специальных символов отсутствуют.\n",
        "\n",
        "special_keys = ['<pad>', '<bos>', '<eos>', '<unk>']\n",
        "\n",
        "for key in special_keys:\n",
        "    if key not in glove_vectors:\n",
        "        print(f'{key} is not present')\n",
        "    else:\n",
        "        print(f'{key}: {glove_vectors[key]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98f89f1b",
      "metadata": {
        "id": "98f89f1b"
      },
      "outputs": [],
      "source": [
        "PAD = '<pad>'\n",
        "BOS = '<bos>'\n",
        "EOS = '<eos>'\n",
        "UNK = '<unk>'\n",
        "\n",
        "special_vecs = {\n",
        "        PAD: np.zeros((50,), dtype=np.float32),\n",
        "        BOS: np.random.normal(size=(50,)).astype(np.float32),\n",
        "        EOS: np.random.normal(size=(50,)).astype(np.float32),\n",
        "        UNK: np.random.normal(size=(50,)).astype(np.float32),\n",
        "}\n",
        "\n",
        "glove_vectors.add_vectors([*special_vecs.keys()], [*special_vecs.values()])\n",
        "glove_vectors.resize_vectors(seed=0)\n",
        "\n",
        "for key in special_keys:\n",
        "    print(glove_vectors.most_similar(key))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145dc753",
      "metadata": {
        "id": "145dc753"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "class Tokenizer():\n",
        "\n",
        "    PAD = '<pad>'\n",
        "    BOS = '<bos>'\n",
        "    EOS = '<eos>'\n",
        "    UNK = '<unk>'\n",
        "    special = [PAD, BOS, EOS, UNK]\n",
        "\n",
        "    def __init__(self, vectors: KeyedVectors, preprocessor = None):\n",
        "\n",
        "        for key in self.special:\n",
        "            if key not in glove_vectors:\n",
        "                raise ValueError(f'Cannot instantiate: special token {key} is not present in given embedding')\n",
        "        self.vectors = vectors\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "        self.PAD_idx = self.vectors.key_to_index[self.PAD]\n",
        "        self.PAD_v = self.vectors[self.PAD_idx]\n",
        "        self.BOS_idx = self.vectors.key_to_index[self.BOS]\n",
        "        self.BOS_v = self.vectors[self.BOS_idx]\n",
        "        self.EOS_idx = self.vectors.key_to_index[self.EOS]\n",
        "        self.EOS_v = self.vectors[self.EOS_idx]\n",
        "        self.UNK_idx = self.vectors.key_to_index[self.UNK]\n",
        "        self.UNK_v = self.vectors[self.UNK_idx]\n",
        "\n",
        "    def tokenize(self, text: str, preprocess=False):\n",
        "        \"\"\"Converts a string to a list of token indices, adding BOS and EOS tokens.\"\"\"\n",
        "        if preprocess:\n",
        "            text = self.preprocessor(text)\n",
        "        tokens = [self.BOS_idx]\n",
        "        # Handle empty words that can result from split\n",
        "        words = [word for word in text.split(' ') if word]\n",
        "        tokens.extend(\n",
        "            self.vectors.key_to_index.get(word, self.UNK_idx)\n",
        "            for word in words\n",
        "        )\n",
        "        tokens.append(self.EOS_idx)\n",
        "        return tokens\n",
        "\n",
        "    def detokenize(self, idxs: list):\n",
        "\n",
        "        return ''.join([self.vectors.index_to_key[i]+' ' for i in idxs]).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b8c0de",
      "metadata": {
        "id": "02b8c0de"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(glove_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57d22edc",
      "metadata": {
        "id": "57d22edc"
      },
      "outputs": [],
      "source": [
        "text = 'abra cadabra'\n",
        "seq = tokenizer.tokenize(text)\n",
        "print(seq)\n",
        "print(tokenizer.detokenize(seq))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f7bf38d",
      "metadata": {
        "id": "8f7bf38d"
      },
      "outputs": [],
      "source": [
        "text = 'NLP (Neuro-Linguistic Programming) is a psychological approach that involves analyzing the patterns of thought, language, and behavior to understand how they interact with and influence human experience. There is no scientific evidence supporting the effectiveness of NLP; it is recognized as a pseudoscience.'\n",
        "seq = tokenizer.tokenize(text)\n",
        "print(seq)\n",
        "print(tokenizer.detokenize(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9be2b19",
      "metadata": {
        "id": "d9be2b19"
      },
      "outputs": [],
      "source": [
        "text = 'General Relativity is a physical theory, which explains gravity as purely geometrical effect: curved spacetime tells matter how to move, while matter influences the curvature of spacetime'\n",
        "seq = tokenizer.tokenize(text)\n",
        "print(seq)\n",
        "print(tokenizer.detokenize(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "332bd2a1",
      "metadata": {
        "id": "332bd2a1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from string import punctuation\n",
        "\n",
        "def text_cleanup(text: str):\n",
        "    text = text.lower()\n",
        "    # Удаляем двойные знаки препинания\n",
        "    text = re.sub(r'([,.])\\1+', r'\\1', text)\n",
        "    # Окружаем знаки препинания пробелами\n",
        "    text = re.sub(f'([{punctuation}])', r' \\1 ', text)\n",
        "    # Удаляем лишние пробелы\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "text_cleanup('sample text: with commas,  double  spaces  ([[and brackets]])..!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b44d70d",
      "metadata": {
        "id": "2b44d70d"
      },
      "outputs": [],
      "source": [
        "tokenizer.preprocessor = text_cleanup\n",
        "text = 'General Relativity is a physical theory, which explains gravity as purely geometrical effect: curved spacetime tells matter how to move, while matter influences the curvature of spacetime'\n",
        "seq = tokenizer.tokenize(text, preprocess=True)\n",
        "print(seq)\n",
        "print(tokenizer.detokenize(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa382b60",
      "metadata": {
        "id": "aa382b60"
      },
      "source": [
        "## 2. Загрузка и предобработка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TxAD1EIGbX5q",
      "metadata": {
        "id": "TxAD1EIGbX5q"
      },
      "outputs": [],
      "source": [
        "!mkdir ./data\n",
        "!curl -L -o ./data/recipes-dataset-64k-dishes.zip\\\n",
        "  https://www.kaggle.com/api/v1/datasets/download/prashantsingh001/recipes-dataset-64k-dishes\n",
        "!unzip ./data/recipes-dataset-64k-dishes.zip -d ./data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cc9d8d8",
      "metadata": {
        "id": "5cc9d8d8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# В общем случае не очень хорошо\n",
        "data_path = './data/1_Recipe_csv.csv'\n",
        "dataframe = pd.read_csv(data_path)\n",
        "dataframe = dataframe.dropna().reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M66ugflz8uTc",
      "metadata": {
        "id": "M66ugflz8uTc"
      },
      "outputs": [],
      "source": [
        "dataframe.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed0ecb5a",
      "metadata": {
        "id": "ed0ecb5a"
      },
      "outputs": [],
      "source": [
        "dataframe.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fpU41xRl83b0",
      "metadata": {
        "id": "fpU41xRl83b0"
      },
      "outputs": [],
      "source": [
        "dataframe['category'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WGQArq978bvN",
      "metadata": {
        "id": "WGQArq978bvN"
      },
      "outputs": [],
      "source": [
        "NCLASSES = 16\n",
        "most_commons = dataframe['category'].value_counts()[:NCLASSES]\n",
        "categories = list(most_commons.index)\n",
        "most_commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X6J6qdKD6lYf",
      "metadata": {
        "id": "X6J6qdKD6lYf"
      },
      "outputs": [],
      "source": [
        "df_reduced = dataframe[dataframe['category'].isin(categories)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w94QmDg1-px6",
      "metadata": {
        "id": "w94QmDg1-px6"
      },
      "outputs": [],
      "source": [
        "df_reduced.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DGTYnb9D6hbc",
      "metadata": {
        "id": "DGTYnb9D6hbc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7b8d556",
      "metadata": {
        "id": "a7b8d556"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def process_recipe(row: pd.Series, columns_to_use: list[str]):\n",
        "    \"\"\"\n",
        "    Processes a single recipe row from the DataFrame into a clean string.\n",
        "    \"\"\"\n",
        "    entry_parts = []\n",
        "    for col in columns_to_use:\n",
        "        if col in row and pd.notna(row[col]):\n",
        "            content = str(row[col])\n",
        "            if isinstance(content, str) and content.startswith('[') and content.endswith(']'):\n",
        "                content = re.sub(r'[\"\\\\\\[\\\\\\\\\\]]', '', content)\n",
        "            entry_parts.append(f'{col.replace(\"_\", \" \")}: {content}')\n",
        "            entry_parts.append(' ')\n",
        "\n",
        "    return ''.join(entry_parts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61638669",
      "metadata": {
        "id": "61638669"
      },
      "outputs": [],
      "source": [
        "COLUMNS = [\n",
        "    'recipe_title',\n",
        "    'description',\n",
        "    'ingredients',\n",
        "    # 'directions'\n",
        "    ]\n",
        "\n",
        "process_recipe(dataframe.iloc[0], COLUMNS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10994d11",
      "metadata": {
        "id": "10994d11"
      },
      "outputs": [],
      "source": [
        "seq = tokenizer.tokenize(process_recipe(dataframe.iloc[0], COLUMNS), preprocess=True)\n",
        "print(seq)\n",
        "print(tokenizer.detokenize(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4402c90",
      "metadata": {
        "id": "e4402c90"
      },
      "source": [
        "## 3. Контейнеры данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d7a5a6",
      "metadata": {
        "id": "47d7a5a6"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "Inpt = List\n",
        "'''Input vector'''\n",
        "Tgt = List | torch.Tensor\n",
        "'''Output vector (can take a form of List or torch.Tensor)'''\n",
        "\n",
        "class RecipeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, df, columns, cat_vec, tokenizer, device='cpu', max_len=256):\n",
        "\n",
        "        self.data_: List[Inpt] = []\n",
        "        self.tgt_: List[Tgt] = []\n",
        "        self.columns = columns\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.PAD_idx = (tokenizer.PAD_idx)\n",
        "        self.max_len = max_len\n",
        "        self.idtype: torch.dtype = torch.int32\n",
        "        self.fdtype: torch.dtype = torch.float32\n",
        "\n",
        "        for i, row in df.iterrows():\n",
        "            entry = process_recipe(row, columns)\n",
        "            tokenized_recipe = tokenizer.tokenize(entry, preprocess=True)\n",
        "\n",
        "            if len(tokenized_recipe) > 10:\n",
        "                self.tgt_.append(cat_vec[i].tolist())\n",
        "                self.data_.append(tokenized_recipe)\n",
        "\n",
        "        self.size = len(self.data_)\n",
        "        # self.subsample()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[Inpt, Tgt]:\n",
        "        return self.data_[idx], self.tgt_[idx]\n",
        "\n",
        "    # def subsample(self, last_idx=None, stride=1):\n",
        "    #     if last_idx == None:\n",
        "    #       last_idx = self.max_size\n",
        "    #     else:\n",
        "    #       last_idx = min(last_idx, self.max_size)\n",
        "\n",
        "    #     self.data = self.data_[:last_idx:stride]\n",
        "    #     self.tgt = self.data_[:last_idx:stride]\n",
        "    #     self.size = len(self.data)\n",
        "\n",
        "    def collate_fn(self, batch: List[Tuple[Inpt, Tgt]]):\n",
        "        seq_lenghts = [len(x[0]) for x in batch]\n",
        "        new_len = min(max(seq_lenghts), self.max_len)\n",
        "\n",
        "        new_inp = []\n",
        "        for i, seq_len in enumerate(seq_lenghts):\n",
        "            _inp_ = batch[i][0][:new_len]\n",
        "            if seq_len < new_len:\n",
        "                _inp_.extend([self.PAD_idx]*(new_len - seq_len))\n",
        "            new_inp.append(_inp_)\n",
        "\n",
        "        # Для RNN размерность входа выбирают (seq_len, batch_size ...)\n",
        "        # либо (batch_size, seq_len, ...). Будем использовать второй вариант.\n",
        "        new_inp = torch.tensor(new_inp, dtype=self.idtype, device=self.device)\n",
        "        new_tgt = torch.tensor([v[1] for v in batch], dtype=self.fdtype, device=self.device)\n",
        "\n",
        "        return (new_inp, new_tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5605064",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e5605064",
        "outputId": "8a732e4a-3b31-4446-d812-1d6b9e3d4851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1600\n",
            "Data split:\n",
            "Training set size: 1280\n",
            "Validation set size: 160\n",
            "Test set size: 160\n",
            "Number of samples:\n",
            "Training: 1280\n",
            "Validation: 160\n",
            "Test: 160\n",
            "Using batching with padding. Ensure your training loop can handle batched data!\n",
            "DataLoaders created with batch size 8.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def get_dataloaders(df, columns_to_use, tokenizer, batch_size=1):\n",
        "  \"\"\"\n",
        "  Loads data, splits it, and creates train, validation, and test DataLoaders.\n",
        "  \"\"\"\n",
        "  cat_encoder = OneHotEncoder(sparse_output=False)\n",
        "  cat_vec = cat_encoder.fit_transform(df[['category']])\n",
        "  indices = list(range(len(df)))\n",
        "  print(len(cat_vec))\n",
        "  random.shuffle(indices)\n",
        "\n",
        "  train_ratio = 0.8\n",
        "  val_ratio = 0.1\n",
        "\n",
        "  train_end = int(len(indices) * train_ratio)\n",
        "  val_end = int(len(indices) * (train_ratio + val_ratio))\n",
        "\n",
        "  train_indices = indices[:train_end]\n",
        "  val_indices = indices[train_end:val_end]\n",
        "  test_indices = indices[val_end:]\n",
        "\n",
        "  train_df = df.iloc[train_indices].reset_index(drop=True)\n",
        "  val_df = df.iloc[val_indices].reset_index(drop=True)\n",
        "  test_df = df.iloc[test_indices].reset_index(drop=True)\n",
        "\n",
        "\n",
        "  print(f\"Data split:\")\n",
        "  print(f\"Training set size: {len(train_df)}\")\n",
        "  print(f\"Validation set size: {len(val_df)}\")\n",
        "  print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "  train_dataset = RecipeDataset(train_df, columns_to_use, cat_vec[train_indices], tokenizer)\n",
        "  val_dataset = RecipeDataset(val_df, columns_to_use, cat_vec[val_indices], tokenizer)\n",
        "  test_dataset = RecipeDataset(test_df, columns_to_use, cat_vec[test_indices], tokenizer)\n",
        "\n",
        "  print(f\"Number of samples:\")\n",
        "  print(f\"Training: {len(train_dataset)}\")\n",
        "  print(f\"Validation: {len(val_dataset)}\")\n",
        "  print(f\"Test: {len(test_dataset)}\")\n",
        "\n",
        "\n",
        "  if batch_size >= 1:\n",
        "      print(\"Using batching with padding. Ensure your training loop can handle batched data!\")\n",
        "\n",
        "  train_loader = DataLoader(train_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=True,\n",
        "                            collate_fn=train_dataset.collate_fn\n",
        "                            )\n",
        "\n",
        "  val_loader = DataLoader(val_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False,\n",
        "                          collate_fn=val_dataset.collate_fn\n",
        "                          )\n",
        "\n",
        "  test_loader = DataLoader(test_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=False,\n",
        "                           collate_fn=test_dataset.collate_fn\n",
        "                            )\n",
        "\n",
        "  print(f\"DataLoaders created with batch size {batch_size}.\")\n",
        "\n",
        "  return train_loader, val_loader, test_loader, cat_encoder\n",
        "df = df_reduced.sample(1600, random_state=42)\n",
        "train_loader, val_loader, test_loader, cat_encoder = get_dataloaders(df, COLUMNS, tokenizer, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f6eb3a8",
      "metadata": {
        "id": "3f6eb3a8"
      },
      "source": [
        "## 4. Классификация текста"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f22435dc",
      "metadata": {
        "id": "f22435dc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Iterable\n",
        "\n",
        "class logger:\n",
        "    active = False\n",
        "    _calls_ = {}\n",
        "    log_file = \"logger_output.txt\"  # Default log file path\n",
        "    silent = True\n",
        "\n",
        "    @classmethod\n",
        "    def on(cls): cls.active = True\n",
        "\n",
        "    @classmethod\n",
        "    def off(cls): cls.active = False\n",
        "\n",
        "    @classmethod\n",
        "    def silent(cls, silent: bool = True):\n",
        "        cls.silent = silent\n",
        "\n",
        "    @classmethod\n",
        "    def zero(cls):\n",
        "        cls._calls_ = {}\n",
        "\n",
        "    @classmethod\n",
        "    def clear_log(cls):\n",
        "        with open(cls.log_file, 'w') as f:\n",
        "            f.write(\"\")\n",
        "\n",
        "    @classmethod\n",
        "    def write_log(cls, msg):\n",
        "        with open(cls.log_file, 'a') as f:\n",
        "            f.write(msg + \"\\n\")\n",
        "\n",
        "    @classmethod\n",
        "    def trace(cls, name):\n",
        "        def log_fn(func):\n",
        "            def wrapper(*args, **kwargs):\n",
        "                if cls.active:\n",
        "                    if name not in cls._calls_:\n",
        "                        cls._calls_[name] = 0\n",
        "                    msg = f'>>> {name} call {cls._calls_[name]}: \\n Args: \\n'\n",
        "                    for i, arg in enumerate(args):\n",
        "                        if isinstance(arg, torch.Tensor):\n",
        "                            msg += f'\\t arg[{i}]: shape={arg.shape}, dtype={arg.dtype}, device={arg.device}\\n {arg} \\n'\n",
        "                        else:\n",
        "                            msg += f'\\t arg[{i}]: {arg}\\n'\n",
        "\n",
        "                    for k, arg in kwargs.items():\n",
        "                        if isinstance(arg, torch.Tensor):\n",
        "                            msg += f'\\t kwarg[{k}]: shape={arg.shape}, dtype={arg.dtype}, device={arg.device}\\n {arg} \\n'\n",
        "                        else:\n",
        "                            msg += f'\\t kwarg[{k}]: {arg} \\n'\n",
        "\n",
        "                    if not cls.silent: print(msg)\n",
        "                    cls.write_log(msg)\n",
        "\n",
        "                result = func(*args, **kwargs)\n",
        "\n",
        "                if cls.active:\n",
        "                    msg = f'Result: \\n'\n",
        "                    if isinstance(result, Iterable):\n",
        "                        for i, outp in enumerate(result):\n",
        "                            if isinstance(outp, torch.Tensor):\n",
        "                                msg += f'\\t output[{i}]: shape={outp.shape}, dtype={outp.dtype}, device={outp.device}\\n {outp} \\n'\n",
        "                            else:\n",
        "                                msg += f'\\t output[{i}]: {outp}\\n'\n",
        "                    elif isinstance(result, torch.Tensor):\n",
        "                        msg = f'\\t output: shape={result.shape}, dtype={result.dtype}, device={result.device}\\n {result} \\n'\n",
        "                    else:\n",
        "                        msg += f'\\t output: {result}\\n'\n",
        "\n",
        "                    if not cls.silent: print(msg)\n",
        "                    cls.write_log(msg)\n",
        "                    cls._calls_[name] += 1\n",
        "\n",
        "                return result\n",
        "            return wrapper\n",
        "\n",
        "        return log_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0IGOSWcQWIOa",
      "metadata": {
        "id": "0IGOSWcQWIOa"
      },
      "outputs": [],
      "source": [
        "embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(glove_vectors.vectors),\n",
        "            freeze=True\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8917c629",
      "metadata": {
        "id": "8917c629"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder module: Embedding + GRU. Returns outputs and hidden state.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding, hidden_size, n_layers=1, dropout=0.5, bidirectional=True, freeze_emb=True):\n",
        "        super().__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.rnn = nn.GRU(\n",
        "            embedding.embedding_dim,\n",
        "            hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if n_layers > 1 else 0\n",
        "            )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.max_len = 256\n",
        "\n",
        "    @logger.trace('ENC')\n",
        "    def forward(self, text, text_lengths = None):\n",
        "        if text_lengths is None:\n",
        "            text_lengths = [len(t) for t in text]\n",
        "        X = self.dropout(self.embedding(text))\n",
        "        X_packed = nn.utils.rnn.pack_padded_sequence(X, text_lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, hidden = self.rnn(X_packed)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "        return output, hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9dbf63a",
      "metadata": {
        "id": "f9dbf63a"
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A text classification model without an attention mechanism, using Encoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding, hidden_size, num_classes, n_layers=1, dropout=0.5, bidirectional=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(embedding, hidden_size, n_layers, dropout, bidirectional)\n",
        "        linear_input_size = hidden_size * 2 if bidirectional else hidden_size\n",
        "        self.fc = nn.Linear(linear_input_size, num_classes)\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "    @logger.trace('CLSF')\n",
        "    def forward(self, text, text_lengths = None):\n",
        "\n",
        "        _, hidden = self.encoder(text, text_lengths)\n",
        "        if self.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2, :,:], hidden[-1,:,:]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[-1,:,:]\n",
        "\n",
        "        return F.softmax(self.fc(hidden), dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f0a5ae",
      "metadata": {
        "id": "e0f0a5ae"
      },
      "outputs": [],
      "source": [
        "import torchinfo\n",
        "# Гиперпараметры моделей\n",
        "HIDDEN_SIZE = 50\n",
        "N_LAYERS = 1\n",
        "DROPOUT = 0.2\n",
        "\n",
        "modelA = TextClassifier(embedding, HIDDEN_SIZE, NCLASSES, N_LAYERS, DROPOUT)\n",
        "torchinfo.summary(modelA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51ef49d9",
      "metadata": {
        "id": "51ef49d9"
      },
      "outputs": [],
      "source": [
        "# Пример входных данных\n",
        "sample_text = [\"this is a test recipe\"]\n",
        "tokenized = [tokenizer.tokenize(sample, preprocess=True) for sample in sample_text]\n",
        "input_tensor = torch.tensor(tokenized, dtype=torch.long)  # (batch, seq_len)\n",
        "text_lengths = torch.tensor([len(seq) for seq in tokenized], dtype=torch.long)  # (batch,)\n",
        "\n",
        "# Вызов модели\n",
        "modelA.eval()\n",
        "logger.clear_log()\n",
        "logger.on()\n",
        "with torch.no_grad():\n",
        "    y_pred = modelA(input_tensor, text_lengths)\n",
        "    print(y_pred)\n",
        "    print(f\"Predicted class: {cat_encoder.inverse_transform(y_pred)}\" )\n",
        "logger.off()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ed4debd",
      "metadata": {
        "id": "1ed4debd"
      },
      "outputs": [],
      "source": [
        "# Цикл обучения\n",
        "from typing import Dict\n",
        "import tqdm\n",
        "\n",
        "class Helper:\n",
        "\n",
        "    eval_fn = None\n",
        "    eval_int = 1\n",
        "    in_notebook = True\n",
        "\n",
        "    @classmethod\n",
        "    def plot_history(cls, history, model=None):\n",
        "\n",
        "        if cls.in_notebook:\n",
        "            from IPython.display import clear_output\n",
        "            clear_output(wait=True)\n",
        "        else:\n",
        "            plt.close('all')\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        axes[0].plot(history['train_loss'], label='train loss')\n",
        "        axes[0].set_xlabel('Epochs')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].legend()\n",
        "\n",
        "        axes[1].plot(history['epochs'], history['train_score'], label='train score')\n",
        "        axes[1].plot(history['epochs'], history['val_score'], label='val score')\n",
        "        axes[1].set_xlabel('Epochs')\n",
        "        axes[1].set_ylabel('Score')\n",
        "        axes[1].legend()\n",
        "\n",
        "        if hasattr(model, 'vis_ax') and model.vis_ax is not None:\n",
        "            fig.axes.append(model.vis_ax)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if cls.in_notebook:\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.pause(0.001)\n",
        "\n",
        "    @classmethod\n",
        "    def train(cls,\n",
        "              model,\n",
        "              optimizer,\n",
        "              dataloaders: Dict[str, DataLoader],\n",
        "              loss,\n",
        "              n_epochs = 30,\n",
        "              plot = False,\n",
        "             ):\n",
        "\n",
        "        history = {'train_loss': [], 'train_score':[], 'val_score': [], 'epochs': []}\n",
        "        if not cls.in_notebook: plt.ion()\n",
        "\n",
        "        for epoch in tqdm.trange(n_epochs):\n",
        "            model.train()\n",
        "            history['train_loss'].append(0)\n",
        "\n",
        "            for X_batch, y_batch in dataloaders['train']:\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                y_pred = model(X_batch)\n",
        "\n",
        "                loss_train = loss(y_pred, y_batch)\n",
        "                history['train_loss'][-1] += loss_train.item()\n",
        "\n",
        "                loss_train.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            if (epoch % cls.eval_int) == 0:\n",
        "                cls.evaluation(model, dataloaders, loss, loss_train, history, epoch)\n",
        "\n",
        "            if plot:\n",
        "              cls.plot_history(history, model)\n",
        "\n",
        "        return history\n",
        "\n",
        "    @classmethod\n",
        "    def evaluation(cls, model, dataloaders, loss, loss_train, history, epoch):\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            eval_test = cls.eval_fn_dl(model, dataloaders['val'])\n",
        "            eval_train = 0\n",
        "            if cls.eval_fn == None:\n",
        "                eval_train = loss_train.item()\n",
        "            else:\n",
        "                eval_train = cls.eval_fn_dl(model, dataloaders['train'])\n",
        "\n",
        "        history['epochs'].append(epoch)\n",
        "        history['train_score'].append(eval_train)\n",
        "        history['val_score'].append(eval_test)\n",
        "\n",
        "    @classmethod\n",
        "    def eval_fn_dl(cls, model, loader, eval_fn = None):\n",
        "\n",
        "        model.eval()\n",
        "        score = 0\n",
        "        for X_batch, y_batch in loader:\n",
        "            y_pred = model(X_batch)\n",
        "            score += cls.eval_fn(y_pred, y_batch).item()\n",
        "\n",
        "        return score / len(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b4aeff",
      "metadata": {
        "id": "e2b4aeff"
      },
      "outputs": [],
      "source": [
        "class Scores:\n",
        "\n",
        "    threshold = 0.7\n",
        "    eps = 1e-8\n",
        "\n",
        "    @classmethod\n",
        "    def accuracy(cls, y_true, y_pred):\n",
        "\n",
        "        return ((y_pred > cls.threshold) == y_true.bool()).float().mean()\n",
        "\n",
        "    @classmethod\n",
        "    def f1_score(cls, y_true, y_pred):\n",
        "\n",
        "        tp = (y_pred * y_true).sum().to(torch.float32)\n",
        "        tn = ((1 - y_pred) * (1 - y_true)).sum().to(torch.float32)\n",
        "        fp = (y_pred * (1 - y_true)).sum().to(torch.float32)\n",
        "        fn = ((1 - y_pred) * y_true).sum().to(torch.float32)\n",
        "\n",
        "        precision = tp / (tp + fp + cls.eps)\n",
        "        recall = tp / (tp + fn + cls.eps)\n",
        "        f1 = 2 * (precision * recall) / (precision + recall + cls.eps)\n",
        "\n",
        "        return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19743039",
      "metadata": {
        "id": "19743039"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.RMSprop(modelA.parameters(), lr=3e-2)\n",
        "dataloaders = {'train': train_loader, 'val': val_loader, 'test': test_loader}\n",
        "loss = torch.nn.KLDivLoss()\n",
        "Helper.eval_fn = Scores.f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55481a21",
      "metadata": {
        "id": "55481a21"
      },
      "outputs": [],
      "source": [
        "_ = Helper.train(modelA, optimizer, dataloaders, loss, n_epochs=10, plot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b38f33b",
      "metadata": {
        "id": "2b38f33b"
      },
      "outputs": [],
      "source": [
        "Helper.eval_fn_dl(modelA, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zq7CTVbEZSR5",
      "metadata": {
        "id": "Zq7CTVbEZSR5"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    X, y_true = test_loader.dataset[1]\n",
        "    y_pred = modelA(torch.IntTensor(X))\n",
        "    print(y_pred.shape)\n",
        "    print(f\"True class: {cat_encoder.inverse_transform(y_true)}\")\n",
        "    print(f\"Predicted class: {cat_encoder.inverse_transform(y_pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O3fj78YorI3C",
      "metadata": {
        "id": "O3fj78YorI3C"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
        "        super().__init__()\n",
        "        self.W_a = nn.Linear(decoder_hidden_dim, decoder_hidden_dim, bias=False)\n",
        "        self.U_a = nn.Linear(encoder_hidden_dim, decoder_hidden_dim, bias=False)\n",
        "        self.v = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
        "\n",
        "    @logger.trace('ATN')\n",
        "    def forward(self, decoder_hidden, encoder_hidden):\n",
        "\n",
        "        s = self.W_a(decoder_hidden).unsqueeze(1)\n",
        "        h = self.U_a(encoder_hidden)\n",
        "        # print(s.shape, h.shape)\n",
        "        scores = self.v(F.tanh(s+h))\n",
        "        attention_weights = F.softmax(scores, dim=1).squeeze(-1)\n",
        "        context_vector = torch.einsum('bs,bsh->bh', attention_weights, encoder_hidden)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "class AttentionTextClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, embeddings, hidden_size, num_classes, window_size=10, stride=2, n_layers=1, dropout=0.5, bidirectional=True):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.encoder = Encoder(embeddings, hidden_size, n_layers, dropout, bidirectional)\n",
        "        rnn_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
        "        self.attention = BahdanauAttention(rnn_output_size, rnn_output_size)\n",
        "        self.fc = nn.Linear(rnn_output_size, num_classes)\n",
        "\n",
        "    def forward(self, text, text_lengths=None):\n",
        "        output, _ = self.encoder(text, text_lengths)\n",
        "        batch_size, seq_length, _ = output.shape\n",
        "        context_vectors = []\n",
        "\n",
        "        for i in range(0, seq_length, self.window_size):\n",
        "            window = output[:, i:i + self.window_size, :]\n",
        "            decoder_hidden = output[:, min(i + self.window_size - 1, seq_length - 1), :]\n",
        "            context_vector, _ = self.attention(decoder_hidden, window)\n",
        "            context_vectors.append(context_vector)\n",
        "\n",
        "        context_vectors = torch.stack(context_vectors, dim=1)\n",
        "        context_vector = context_vectors.mean(dim=1)\n",
        "        output = self.fc(context_vector)\n",
        "        return F.softmax(output, dim=1)\n",
        "\n",
        "modelB = AttentionTextClassifier(embedding, HIDDEN_SIZE, NCLASSES, 10, 2, N_LAYERS, DROPOUT)\n",
        "torchinfo.summary(modelB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deb50ad7",
      "metadata": {
        "id": "deb50ad7"
      },
      "outputs": [],
      "source": [
        "# Пример входных данных\n",
        "sample_text = [\"this is a test recipe, very long test recipe !\"]\n",
        "tokenized = [tokenizer.tokenize(sample, preprocess=True) for sample in sample_text]\n",
        "input_tensor = torch.tensor(tokenized, dtype=torch.long)  # (batch, seq_len)\n",
        "text_lengths = torch.tensor([len(seq) for seq in tokenized], dtype=torch.long)  # (batch,)\n",
        "\n",
        "# Вызов модели\n",
        "modelB.eval()\n",
        "logger.zero()\n",
        "logger.clear_log()\n",
        "logger.on()\n",
        "with torch.no_grad():\n",
        "    y_pred = modelB(input_tensor, text_lengths)\n",
        "    print(y_pred)\n",
        "    print(f\"Predicted class: {cat_encoder.inverse_transform(y_pred)}\" )\n",
        "logger.off()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l8Tc4ttzjPBL",
      "metadata": {
        "id": "l8Tc4ttzjPBL"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.RMSprop(modelB.parameters(), lr=3e-4)\n",
        "dataloaders = {'train': train_loader, 'val': val_loader, 'test': test_loader}\n",
        "loss = torch.nn.KLDivLoss()\n",
        "Helper.eval_fn = Scores.f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b12d063",
      "metadata": {
        "id": "8b12d063"
      },
      "outputs": [],
      "source": [
        "_ = Helper.train(modelB, optimizer, dataloaders, loss, n_epochs=5, plot=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nn_env_2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}