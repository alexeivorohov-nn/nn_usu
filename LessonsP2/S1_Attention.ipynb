{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Iterable, Sized\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")\n",
    "\n",
    "# Зафиксируем зерна\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "class logger:\n",
    "    active = False\n",
    "    _calls_ = {}\n",
    "    log_file = \"logger_output.txt\"  # Default log file path\n",
    "    silent = True\n",
    "\n",
    "    @classmethod\n",
    "    def on(cls): cls.active = True\n",
    "\n",
    "    @classmethod\n",
    "    def off(cls): cls.active = False\n",
    "\n",
    "    @classmethod\n",
    "    def silent(cls, silent: bool = True):\n",
    "        cls.silent = silent\n",
    "\n",
    "    @classmethod\n",
    "    def zero(cls):\n",
    "        cls._calls_ = {}\n",
    "\n",
    "    @classmethod\n",
    "    def clear_log(cls):\n",
    "        with open(cls.log_file, 'w') as f:\n",
    "            f.write(\"\")\n",
    "\n",
    "    @classmethod\n",
    "    def write_log(cls, msg):\n",
    "        with open(cls.log_file, 'a') as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "\n",
    "    @classmethod\n",
    "    def trace(cls, name):\n",
    "        def log_fn(func):\n",
    "            def wrapper(*args, **kwargs):\n",
    "                if cls.active:\n",
    "                    if name not in cls._calls_:\n",
    "                        cls._calls_[name] = 0\n",
    "                    msg = f'>>> {name} call {cls._calls_[name]}: \\n Args: \\n'\n",
    "                    for i, arg in enumerate(args):\n",
    "                        if isinstance(arg, torch.Tensor):\n",
    "                            msg += f'\\t arg[{i}]: shape={arg.shape}, dtype={arg.dtype}, device={arg.device}\\n {arg} \\n'\n",
    "                        else:\n",
    "                            msg += f'\\t arg[{i}]: {arg}\\n'\n",
    "\n",
    "                    for k, arg in kwargs.items():\n",
    "                        if isinstance(arg, torch.Tensor):\n",
    "                            msg += f'\\t kwarg[{k}]: shape={arg.shape}, dtype={arg.dtype}, device={arg.device}\\n {arg} \\n'\n",
    "                        else:\n",
    "                            msg += f'\\t kwarg[{k}]: {arg} \\n'\n",
    "\n",
    "                    if not cls.silent: print(msg)\n",
    "                    cls.write_log(msg)\n",
    "\n",
    "                result = func(*args, **kwargs)\n",
    "\n",
    "                if cls.active:\n",
    "                    msg = f'Result: \\n'\n",
    "                    if isinstance(result, Iterable):\n",
    "                        for i, outp in enumerate(result):\n",
    "                            if isinstance(outp, torch.Tensor):\n",
    "                                msg += f'\\t output[{i}]: shape={outp.shape}, dtype={outp.dtype}, device={outp.device}\\n {outp} \\n'\n",
    "                            else:\n",
    "                                msg += f'\\t output[{i}]: {outp}\\n'\n",
    "                    elif isinstance(result, torch.Tensor):\n",
    "                        msg = f'\\t output: shape={result.shape}, dtype={result.dtype}, device={result.device}\\n {result} \\n'\n",
    "                    else:\n",
    "                        msg += f'\\t output: {result}\\n'\n",
    "\n",
    "                    if not cls.silent: print(msg)\n",
    "                    cls.write_log(msg)\n",
    "                    cls._calls_[name] += 1\n",
    "\n",
    "                return result\n",
    "            return wrapper\n",
    "\n",
    "        return log_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE3a4sgdCQPr"
   },
   "source": [
    "# 1. Токенизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "# https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mobilebert-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "print(f'Vocabulary size: {VOCAB_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS token  is not present in tokenizer\n",
      "EOS token  is not present in tokenizer\n",
      "PAD token: token [PAD], id 0\n",
      "UNK token: token [UNK], id 100\n",
      "SEP token: token [SEP], id 102\n",
      "CLS token: token [CLS], id 101\n"
     ]
    }
   ],
   "source": [
    "def collect_special_tokens(tokenizer: AutoTokenizer) -> Dict:\n",
    "\n",
    "    special_tokens = ['PAD', 'BOS', 'EOS', 'UNK', 'SEP', 'CLS']\n",
    "    outp = {}\n",
    "    for name in special_tokens:\n",
    "        token = getattr(tokenizer, f'{name.lower()}_token')\n",
    "        if token is not None:\n",
    "            outp[name] = token\n",
    "            outp[f'{name}_id'] = getattr(tokenizer, f'{name.lower()}_token_id')\n",
    "        else:\n",
    "            print(f'{name} token  is not present in tokenizer')\n",
    "    \n",
    "    for name in special_tokens:\n",
    "        if name in outp:\n",
    "            print(f'{name} token: token {outp[name]}, id {outp[f'{name}_id']}')\n",
    "\n",
    "    return outp\n",
    "\n",
    "special_tokens = collect_special_tokens(tokenizer)\n",
    "\n",
    "for k, v in special_tokens.items():\n",
    "    globals()[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 17953, 2361, 1006, 11265, 10976, 1011, 12158, 4730, 1007, 2003, 1037, 8317, 3921, 2008, 7336, 20253, 1996, 7060, 1997, 2245, 1010, 2653, 1010, 1998, 5248, 2000, 3305, 2129, 2027, 11835, 2007, 1998, 3747, 2529, 3325, 1012, 2045, 2003, 2053, 4045, 3350, 4637, 1996, 12353, 1997, 17953, 2361, 1025, 2009, 2003, 3858, 2004, 1037, 18404, 11020, 13684, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] nlp ( neuro - linguistic programming ) is a psychological approach that involves analyzing the patterns of thought, language, and behavior to understand how they interact with and influence human experience. there is no scientific evidence supporting the effectiveness of nlp ; it is recognized as a pseudoscience. [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = 'NLP (Neuro-Linguistic Programming) is a psychological approach that involves analyzing the patterns of thought, language, and behavior to understand how they interact with and influence human experience. There is no scientific evidence supporting the effectiveness of NLP; it is recognized as a pseudoscience.'\n",
    "# apply tokenizer:\n",
    "seq = tokenizer(text)\n",
    "print(seq)\n",
    "# decode with tokenizer:\n",
    "print(tokenizer.decode(seq['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 17953,  2361,  1006, 11265, 10976,  1011, 12158,  4730,  1007,\n",
      "          2003,  1037,  8317,  3921,  2008,  7336, 20253,  1996,  7060,  1997,\n",
      "          2245,  1010,  2653,  1010,  1998,  5248,  2000,  3305,  2129,  2027,\n",
      "         11835,  2007,  1998,  3747,  2529,  3325,  1012,  2045,  2003,  2053,\n",
      "          4045,  3350,  4637,  1996, 12353,  1997, 17953,  2361,  1025,  2009,\n",
      "          2003,  3858,  2004,  1037, 18404, 11020, 13684,  1012,   102,     0,\n",
      "             0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]), 'length': tensor([64])}\n",
      "[CLS] nlp ( neuro - linguistic programming ) is a psychological approach that involves analyzing the patterns of thought, language, and behavior to understand how they interact with and influence human experience. there is no scientific evidence supporting the effectiveness of nlp ; it is recognized as a pseudoscience. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "texts = ['NLP (Neuro-Linguistic Programming) is a psychological approach that involves analyzing the patterns of thought, language, and behavior to understand how they interact with and influence human experience.',\n",
    "         'There is no scientific evidence supporting the effectiveness of NLP; it is recognized as a pseudoscience.']\n",
    "seq = tokenizer(text, max_length=64, padding=\"max_length\",\\\n",
    "                truncation=\"longest_first\", return_tensors=\"pt\",\\\n",
    "                return_token_type_ids = False, return_length=True)\n",
    "print(seq)\n",
    "# decode with tokenizer:\n",
    "print(tokenizer.decode(seq['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возвращается не словарь, а свой тип, имеющий  методы Dict() и другие \n",
    "type(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] nl ##p ( ne ##uro - linguistic programming ) is a psychological approach that involves analyzing the patterns of thought , language , and behavior to understand how they interact with and influence human experience . there is no scientific evidence supporting the effectiveness of nl ##p ; it is recognized as a pseudo ##sc ##ience . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(*seq.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_MutableMapping__marker   __abstractmethods__   __class__   __class_getitem__   __contains__   __copy__   __delattr__   __delitem__   __dict__   __dir__   __doc__   __eq__   __format__   __ge__   __getattr__   __getattribute__   __getitem__   __getstate__   __gt__   __hash__   __init__   __init_subclass__   __ior__   __iter__   __le__   __len__   __lt__   __module__   __ne__   __new__   __or__   __reduce__   __reduce_ex__   __repr__   __reversed__   __ror__   __setattr__   __setitem__   __setstate__   __sizeof__   __slots__   __str__   __subclasshook__   __weakref__   _abc_impl   _encodings   _n_sequences   char_to_token   char_to_word   clear   convert_to_tensors   copy   data   encodings   fromkeys   get   is_fast   items   keys   n_sequences   pop   popitem   sequence_ids   setdefault   to   token_to_chars   token_to_sequence   token_to_word   tokens   update   values   word_ids   word_to_chars   word_to_tokens   words   \n"
     ]
    }
   ],
   "source": [
    "print(''.join([_attr_+'   ' for _attr_ in dir(seq)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 17953,  2361,  1006, 11265, 10976,  1011, 12158,  4730,  1007,\n",
      "          2003,  1037,  8317,  3921,  2008,  7336, 20253,  1996,  7060,  1997,\n",
      "          2245,  1010,  2653,  1010,  1998,  5248,  2000,  3305,  2129,  2027,\n",
      "         11835,  2007,  1998,  3747,  2529,  3325,  1012,  2045,  2003,  2053,\n",
      "          4045,  3350,  4637,  1996, 12353,  1997, 17953,  2361,  1025,  2009,\n",
      "          2003,  3858,  2004,  1037, 18404, 11020, 13684,  1012,   102,     0,\n",
      "             0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "print(seq.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjcEKe88CU6I"
   },
   "source": [
    "# 2. Данные и контейнеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data directory: ../data\n",
      "Found dataset: ../data/1_Recipe_csv.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "data_dir=\"../data\"\n",
    "csv_name=\"1_Recipe_csv.csv\"\n",
    "zip_name=\"recipes-dataset-64k.zip\"\n",
    "zip_path=\"$data_dir/$zip_name\"\n",
    "\n",
    "if [ ! -d \"$data_dir\" ]; then\n",
    "    mkdir -p \"$data_dir\"\n",
    "    echo \"Directory created: $data_dir\"\n",
    "else\n",
    "    echo \"Found data directory: $data_dir\"\n",
    "fi\n",
    "\n",
    "if [ ! -f \"$data_dir/$csv_name\" ]; then\n",
    "    echo \"CSV file $csv_name not found...\"\n",
    "    if [ ! -f \"$zip_path\" ]; then\n",
    "        echo \"Archive $zip_name not found, downloading...\"\n",
    "        curl -L -o \"$zip_path\" https://www.kaggle.com/api/v1/datasets/download/prashantsingh001/recipes-dataset-64k-dishes\n",
    "        echo \"Dataset downloaded: $zip_path\"\n",
    "    else\n",
    "        echo \"Found archive $zip_name\"\n",
    "    fi\n",
    "    echo \"Extracting to $data_dir ...\"\n",
    "    unzip \"$zip_path\" -d \"$data_dir\"\n",
    "    echo \"Dataset unzipped to: $data_dir\"\n",
    "else\n",
    "    echo \"Found dataset: $data_dir/$csv_name\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:00<00:00, 50289.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class RecipeDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self, path: str,\n",
    "            source_columns: List[str],\n",
    "            target_columns: List[str],\n",
    "            tokenizer: AutoTokenizer,\n",
    "            nrows = None,\n",
    "            device = 'cpu',\n",
    "            max_len = 256,\n",
    "            padding_type = \"max_length\",\n",
    "            ):\n",
    "\n",
    "        columns = source_columns + target_columns\n",
    "        self.source_columns = source_columns\n",
    "        self.target_columns = target_columns\n",
    "\n",
    "        data = pd.read_csv(path, usecols=columns, nrows=nrows)\n",
    "\n",
    "        self.src_ = []\n",
    "        self.tgt_ = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.PAD_idx = tokenizer.pad_token_id\n",
    "        self.max_len = max_len\n",
    "        self.padding_type = padding_type\n",
    "\n",
    "        for i in tqdm.trange(len(data)):\n",
    "            row = data.iloc[i]\n",
    "            src_text = self.process_row(row, source_columns)\n",
    "            tgt_text = self.process_row(row, target_columns)\n",
    "            \n",
    "            self.src_.append(src_text)\n",
    "            self.tgt_.append(tgt_text)\n",
    "\n",
    "        self.size = len(self.src_)\n",
    "\n",
    "    def process_row(self, row: pd.Series, columns: List[str]):\n",
    "        \"\"\"Processes a single recipe row from the DataFrame into a clean string.\"\"\"\n",
    "        entry_parts = []\n",
    "        for col in columns:\n",
    "            if pd.notna(row[col]):\n",
    "                content = str(row[col])\n",
    "                if content.startswith('[') and content.endswith(']'):\n",
    "                    content = re.sub(r'[\"\\\\$$\\\\\\\\$$]', '', content)\n",
    "                entry_parts.append(f'{col.replace(\"_\", \" \")}: {content}')\n",
    "                entry_parts.append('\\n')\n",
    "        return ''.join(entry_parts[:-1])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[str, str]:\n",
    "        source = self.src_[idx]\n",
    "        target = self.tgt_[idx]\n",
    "        return source, target\n",
    "\n",
    "    def single(self, idx):\n",
    "        return self.collate_fn([(self.src_[idx], self.tgt_[idx])])\n",
    "\n",
    "    def collate_fn(self, batch: List[Tuple[str, str]]) -> Dict[str, List]:\n",
    "        \n",
    "        sources = [f[0] for f in batch]\n",
    "        targets = [f[1] for f in batch]\n",
    "    \n",
    "        source_enc = self.tokenizer(list(sources), max_length=self.max_len, \n",
    "                                    padding=self.padding_type, truncation=True, \n",
    "                                    return_tensors=\"pt\", return_length=True)\n",
    "\n",
    "        target_enc = self.tokenizer(list(targets), max_length=self.max_len,\n",
    "                                    padding=self.padding_type, truncation=True, \n",
    "                                    return_tensors=\"pt\", return_length=True)\n",
    "\n",
    "        batch = {\n",
    "            'input_ids': source_enc['input_ids'], \n",
    "            'attention_mask': source_enc['attention_mask'],\n",
    "            'input_lengths': source_enc['length'],\n",
    "            'labels': target_enc['input_ids'],\n",
    "            'labels_lengths': target_enc['length'],\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "DATA_PATH = '../data/1_Recipe_csv.csv'\n",
    "INPUT_COLS = ['recipe_title', 'category']\n",
    "TARGET_COLS = ['description', 'ingredients', 'directions']\n",
    "NSAMPLES = 1024\n",
    "\n",
    "data = RecipeDataset(DATA_PATH, INPUT_COLS, TARGET_COLS,\\\n",
    "                     tokenizer, nrows=NSAMPLES, padding_type=\"longest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe title: Wasabi Shrimp Sushi Cups\n",
      "category: Allrecipes Allstar Recipes\n",
      "description: These wasabi shrimp sushi cups are perfect for you if rolling sushi is not in your wheelhouse. Certainly filling a muffin cup is! Prepare sushi rice according to the package instructions, but do not fluff—the success of this dish depends on very sticky rice. Use a silicone muffin pan, if you have one. While the rice cooks, prepare the filling ingredients.\n",
      "ingredients: [1 tablespoon wasabi paste, 1 teaspoon soy sauce, 2 tablespoons mayonnaise]\n",
      "directions: [For wasabi aioli; combine wasabi paste, soy sauce, and mayonnaise in a small, zippered food storage bag. Gently squeeze the bag to combine ingredients. Taste and add more wasabi paste, if desired. Seal the bag and refrigerate., Place about 1/8 cup cooked and cooled rice in each of 8 silicone muffin cups. Press rice firmly into the cup, using damp fingertips. Shape some of the rice up the sides of the cup. Refrigerate at least 20 minutes., To serve, fill rice cups with matchstick carrots, matchstick cucumbers, and edamame. Place one shrimp half on top of each filled cup., Snip the tip off 1 corner of wasabi aioli bag, and squeeze contents across the top of each filled cup. Sprinkle with black sesame seeds. Serve immediately with pickled ginger.]\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(0, NSAMPLES)\n",
    "print(data[idx][0])\n",
    "print(data[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 17974,  2516,  1024,  2250, 14744,  2121, 14557, 25609,  2007,\n",
       "          23427, 12901,  4696,  1024,  2250, 14744,  2121, 19328,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'input_lengths': tensor([19]),\n",
       " 'labels': tensor([[  101,  6412,  1024,  2122,  2250, 14744,  2121, 14557, 25609,  1010,\n",
       "           2366,  2007,  1037,  5404, 17710, 10649,  6279, 23427, 12901,  1010,\n",
       "           2024,  1037, 11937, 21756,  4344,  2833,  4873,  2090,  1037,  2413,\n",
       "          14744,  1998,  1037, 14557,  9090,  1012,  2079,  2202,  1996,  2051,\n",
       "           2000,  2191,  1996, 23427, 12901,  1517,  2009,  1005,  1055,  4276,\n",
       "           2009,  1012, 12760,  1024,  1031,  1017,  1013,  1018,  2452, 17710,\n",
       "          10649,  6279,  1010,  1015,  1013,  1016,  2452,  5404,  1010,  1015,\n",
       "           7251, 24667,  2078, 20218, 12901,  1010,  1015,  1013,  1016,  5572,\n",
       "          13102,  7828, 20949,  9898,  1010,  1015,  1013,  1018,  5572, 13102,\n",
       "           7828,  6187, 20684,  2638,  1010,  1016, 21522, 14629,  1010,  9724,\n",
       "           3514,  8434, 12509,  1010,  1015,  1013,  1016,  5572, 13102,  7828,\n",
       "          20548,  9898,  1010,  5474,  1998, 20229,  2598,  2304, 11565,  1033,\n",
       "           7826,  1024,  1031, 11506, 17710, 10649,  6279,  1010,  5404,  1010,\n",
       "          20218, 12901,  1010, 20949,  9898,  1010,  1998,  6187, 20684,  2638,\n",
       "           1999,  1037,  2235, 12901,  9739,  1012,  3288,  2000,  1037, 26077,\n",
       "           1010,  2059,  5547,  3684,  1010,  1998, 21934,  5017,  2005,  1017,\n",
       "           2000,  1019,  2781,  1012,  6366,  2013,  3684,  1010,  1998,  4658,\n",
       "           1012,  3104,  1998,  3573,  1999,  1996, 18097,  2127,  3201,  2000,\n",
       "           2224,  1012,  1010,  3653, 20192,  2102,  1996,  2250, 14744,  2121,\n",
       "           2000,  4278,  5445,  1042,  1006,  3263,  5445,  1039,  1007,  1012,\n",
       "          12509,  1996, 10810,  2007,  8434, 12509,  2030,  2240,  2007,  1037,\n",
       "           4487, 13102,  8820,  3468, 22433, 11197,  1012,  1010, 14704, 14629,\n",
       "           1015,  1013,  1018,  1011,  4960,  4317,  1006,  2224,  1037, 17687,\n",
       "           2063,  2065,  2017,  2031,  2028,  1007,  1010,  1998,  2173,  1999,\n",
       "           1037,  4605,  1012, 12509, 14629,  2007,  9724,  3514, 12509,  1010,\n",
       "           1998, 11867,  6657, 19099,  2007,   102]]),\n",
       " 'labels_lengths': tensor([256])}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.single(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split:\n",
      "Training set size: 819\n",
      "Validation set size: 102\n",
      "Test set size: 103\n",
      "Using batching with padding. Ensure your training loop can handle batched data!\n",
      "DataLoaders created with batch size 8.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "def get_dataloaders(dataset: Dataset, collate_fn=None, train_ratio=0.8, val_ratio=0.1, batch_size=1):\n",
    "    \"\"\"\n",
    "    Loads data, splits it, and creates train, validation, and test DataLoaders.\n",
    "    \"\"\"\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    train_end = int(len(indices) * train_ratio)\n",
    "    val_end = int(len(indices) * (train_ratio + val_ratio))\n",
    "\n",
    "    train_set = Subset(dataset, indices[:train_end])\n",
    "    val_set = Subset(dataset, indices[train_end:val_end])\n",
    "    test_set = Subset(dataset, indices[val_end:])\n",
    "\n",
    "    print(f\"Data split:\")\n",
    "    print(f\"Training set size: {len(train_set)}\")\n",
    "    print(f\"Validation set size: {len(val_set)}\")\n",
    "    print(f\"Test set size: {len(test_set)}\")\n",
    "\n",
    "    if batch_size >= 1:\n",
    "        print(\"Using batching with padding. Ensure your training loop can handle batched data!\")\n",
    "\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    val_loader = DataLoader(val_set, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    test_loader = DataLoader(test_set, batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    print(f\"DataLoaders created with batch size {batch_size}.\")\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(data, collate_fn=data.collate_fn, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 17974,  2516,  1024,  2250, 14744,  2121, 13501,  6763,  4696,\n",
      "          1024,  2035,  2890,  6895, 10374,  2035, 14117, 19328,   102,     0,\n",
      "             0,     0],\n",
      "        [  101, 17974,  2516,  1024,  7095,  6548,  2098,  6763,  4696,  1024,\n",
      "          2035,  2890,  6895, 10374,  2035, 14117, 19328,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 17974,  2516,  1024, 20665,  2004, 28689, 12349, 16521,  4696,\n",
      "          1024,  2035,  2890,  6895, 10374,  2035, 14117, 19328,   102,     0,\n",
      "             0,     0],\n",
      "        [  101, 17974,  2516,  1024, 24514,  2100, 24881, 16521,  2007, 23605,\n",
      "         14580, 11225,  4696,  1024,  2035,  2890,  6895, 10374,  2035, 14117,\n",
      "         19328,   102],\n",
      "        [  101, 17974,  2516,  1024,  2190,  2412,  2630,  9766, 16324,  4696,\n",
      "          1024,  2035,  2890,  6895, 10374,  2035, 14117, 19328,   102,     0,\n",
      "             0,     0],\n",
      "        [  101, 17974,  2516,  1024,  1018,  1011, 21774,  7987,  2271, 20318,\n",
      "          2696, 16510,  4696,  1024,  2035,  2890,  6895, 10374,  2035, 14117,\n",
      "         19328,   102],\n",
      "        [  101, 17974,  2516,  1024,  4943,  7975,  2373, 16521,  4696,  1024,\n",
      "          2035,  2890,  6895, 10374,  2035, 14117, 19328,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 17974,  2516,  1024,  2534, 24778,  2072,  6350, 22094,  4696,\n",
      "          1024,  2035,  2890,  6895, 10374,  2035, 14117, 19328,   102,     0,\n",
      "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]), 'input_lengths': tensor([22, 22, 22, 22, 22, 22, 22, 22]), 'labels': tensor([[  101,  6412,  1024,  ...,     0,     0,     0],\n",
      "        [  101,  6412,  1024,  ...,     0,     0,     0],\n",
      "        [  101,  6412,  1024,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  6412,  1024,  ...,     0,     0,     0],\n",
      "        [  101,  6412,  1024,  ..., 29247,  2007,   102],\n",
      "        [  101,  6412,  1024,  ...,  2072, 25609,   102]]), 'labels_lengths': tensor([256, 256, 256, 256, 256, 256, 256, 256])}\n"
     ]
    }
   ],
   "source": [
    "batch_n = 1\n",
    "n = 0\n",
    "\n",
    "for X in test_loader:\n",
    "    if n == batch_n:\n",
    "         print(X)\n",
    "    elif n > batch_n:\n",
    "        break\n",
    "    n += 1      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pgpvoyMCuMq"
   },
   "source": [
    "# 4.1. Seq2seq без внимания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDTYPE = torch.int32\n",
    "FDTYPE = torch.float32\n",
    "# FDTYPE = torch.float8_e5m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, mask, lengths):\n",
    "        \"\"\"\n",
    "        Applies a bidirectional GRU to sequence of embeddings x.\n",
    "        The input mini-batch x needs to be sorted by length.\n",
    "        x should have dimensions [batch, time, dim].\n",
    "        \"\"\"\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
    "        output, hidden = self.rnn(packed)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.5,\n",
    "                 bridge=True):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "                 \n",
    "        self.rnn = nn.GRU(input_size + 2*hidden_size, hidden_size, num_layers,\n",
    "                          batch_first=True, dropout=dropout)\n",
    "                 \n",
    "        # Для инициализации из конечного состояния энкодера\n",
    "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, inputs, encoder_hidden, \n",
    "                src_mask=None, trg_mask=None, hidden=None, max_len=None):\n",
    "        \"\"\"\n",
    "        Unroll the decoder for a batch of text sequences\n",
    "\n",
    "        Args:\n",
    "            - inputs: torch.Tensor - encoder output (batch, seq_len, encoder_output)\n",
    "            - encoder_hidden: torch.Tensor - last encoder hidden state (batch, seq_len, encoder_hidden)\n",
    "        \"\"\"\n",
    "                                        \n",
    "        if max_len is None:\n",
    "            max_len = trg_mask.size(-1)\n",
    "\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(encoder_hidden)\n",
    "            \n",
    "        decoder_outputs = []\n",
    "        decoder_states = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            output, hidden = self.rnn(inputs, hidden)\n",
    "            decoder_states.append(hidden)\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_states = torch.cat(decoder_states, dim=1)\n",
    "        return decoder_states, hidden # [B, L, H]\n",
    "\n",
    "    def init_hidden(self, encoder_final):\n",
    "        \"\"\"Returns the initial decoder state,\n",
    "        conditioned on the final encoder state.\"\"\"\n",
    "\n",
    "        if encoder_final is None:\n",
    "            return None  \n",
    "\n",
    "        return torch.tanh(self.bridge(encoder_final))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Seq2Seq architectuew\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 embed_dim,\n",
    "                 hidden_size,\n",
    "                 n_layers,\n",
    "                 dropout,\n",
    "                 EncoderCls,\n",
    "                 DecoderCls,\n",
    "                 tie_projection=False\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - tokenizer: AutoTokenizer\n",
    "            - embed_dim: int - size of embedding dimension\n",
    "            - nlayers: number of layers in rnn\n",
    "            - cls: encoder class to use, must match signature (),\n",
    "            - decoder_cls: decoder class to use, must match signature (),\n",
    "            - tie_projection: bool - if true, embedding and projection layer will use same weights\n",
    "        \"\"\"\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embeddings(self.vocab_size, embded_dim, padding_idx=0)\n",
    "        self.encoder = EncoderCls(embed_dim, hidden_size, n_layers, dropout)\n",
    "        self.decoder = DecoderCls(hidden_size, embed_dim, n_layers, dropout)\n",
    "        self.projection = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        if tie_projection:\n",
    "            self.projection.weights = self.embedding.weights\n",
    "        \n",
    "    def forward(self, tokens, src_mask=None, src_lengths=None, tgt_mask=None, tgt_lengths=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - tokens: torch.Tensor[int] : input tokens \n",
    "        \"\"\"\n",
    "        embedded = self.embedding(tokens)\n",
    "        encoder_hidden, encoder_final = self.encode(tokens, src_mask, src_lengths)\n",
    "        decoder_hidden, decoder_final = self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
    "\n",
    "        logits\n",
    "        return logits\n",
    "        \n",
    "    def encode(self, src, src_mask, src_lengths):\n",
    "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
    "    \n",
    "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
    "               decoder_hidden=None):\n",
    "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
    "                            src_mask, trg_mask, hidden=decoder_hidden)\n",
    "    \n",
    "    def calculate_loss(self, loss_fn, src_mask, trg_mask, src_lengths, trg_lengths):\n",
    "\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWclVf57Cdjh"
   },
   "source": [
    "# 4.2. Цикл обучения и другие рутины"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8d6zRUeDN-k"
   },
   "source": [
    "# 4.2. Seq2seq с вниманием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySfjzTvvDgBr"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        \n",
    "        # Поскольку в энкодере двунаправленный GRU, key_size = 2*hidden_size\n",
    "        key_size = 2 * hidden_size if key_size is None else key_size\n",
    "        query_size = hidden_size if query_size is None else query_size\n",
    "\n",
    "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
    "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
    "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "        # to store attention scores\n",
    "        self.scores = None\n",
    "        \n",
    "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
    "        assert mask is not None, \"mask is required\"\n",
    "\n",
    "        # We first project the query (the decoder state).\n",
    "        # The projected keys (the encoder states) were already pre-computated.\n",
    "        query = self.query_layer(query)\n",
    "        \n",
    "        # Calculate scores.\n",
    "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        \n",
    "        # Mask out invalid positions.\n",
    "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
    "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        \n",
    "        # Turn scores to probabilities.\n",
    "        self.scores = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # The context vector is the weighted sum of the values.\n",
    "        context = torch.bmm(self.scores, value)\n",
    "        \n",
    "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
    "        return context, self.scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализуйте и обучите Seq2Seq с вниманием\n",
    "\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bp_g1_IkDrGz"
   },
   "source": [
    "# 5. Контрольные вопросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlACumeWDvxV"
   },
   "outputs": [],
   "source": [
    "# Насколько корректно модель предсказывает рецепты из тренировочной выборки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOkcdsQWD_7Y"
   },
   "outputs": [],
   "source": [
    "# Насколько корректно модель предсказывает рецепты из тестовой выборки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5subBxpEH90"
   },
   "outputs": [],
   "source": [
    "# Что будет, если подать рецепт из категорий, на которых модель не обучалась?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zthbgR3lEeao"
   },
   "outputs": [],
   "source": [
    "# Что будет, если подать промпт, не предполагающйий наличие рецепта?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
